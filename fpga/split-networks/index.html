<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://parallel-ml.github.io/docs/fpga/split-networks/">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>FPGA - SARAV Documentaion</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "FPL19 Split Networks on FPGA", url: "#_top", children: [
              {title: "Motivation", url: "#motivation" },
              {title: "Code Description", url: "#code-description" },
              {title: "Setup", url: "#setup" },
              {title: "Usage", url: "#usage" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="../../search/main.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../asplos2018/real-time/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../asplos2018/real-time/" class="btn btn-xs btn-link">
        ASPLOS 2018
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../adhoc/adhoc/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../adhoc/adhoc/" class="btn btn-xs btn-link">
        Adhoc Network
      </a>
    </div>
    
  </div>

    

    <h1 id="fpl19-split-networks-on-fpga">FPL19 Split Networks on FPGA</h1>
<p>Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2 <a href="https://pynq.io">PYNQ FPGA boards</a> using <a href="https://tvm.ai">TVM/VTA</a>.</p>
<p>Github repo is available <a href="https://github.com/parallel-ml/Capella-FPL19-SplitNetworksOnFPGA">here</a>.</p>
<h2 id="motivation">Motivation</h2>
<p>Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs
often require more resources than those provided by individual edge devices.</p>
<p>The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run
separately on multiple edge devices in parallel.</p>
<p>The outputs from the split networks are then
concatenated and fed through the fully connected layers to perform inference.</p>
<h2 id="code-description">Code Description</h2>
<ul>
<li><code>splitnet.py</code> contains split models built with <a href="https://mxnet.incubator.apache.org/versions/master/gluon/index.html">MxNet Gluon</a>. Only <code>resnet18_v1_split</code> is implemented so far.</li>
<li><code>resnet18_v1_split</code> returns a split version of <code>mxnet.gluon.model_zoo.vision.resnet18_v1</code>; initialized with random weights.</li>
<li><code>demo.py</code> demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results.</li>
<li><code>autotune.py</code> uses TVM's autotuning tool to achieve fast performance when running <code>resnet18_v1_split</code> on PYNQ FPGA. Currently broken.</li>
</ul>
<h2 id="setup">Setup</h2>
<h3 id="pynq-boards">PYNQ Boards</h3>
<p>To deploy the split networks, first acquire 2 PYNQ boards
and set them up following instructions <a href="https://pynq.readthedocs.io/en/latest/getting_started/pynq_z1_setup.html">here</a>.</p>
<p>After PYNQ boards are set up, follow instructions <a href="https://docs.tvm.ai/vta/install.html#pynq-side-rpc-server-build-deployment">here</a> to
launch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server:</p>
<pre><code>INFO:root:RPCServer: bind to 0.0.0.0:9091
</code></pre>

<p>The RPC server should be listening on port <code>9091</code>.</p>
<h3 id="local">Local</h3>
<p>The following instructions apply to your local machine. CNN models are developed, compiled
&amp; uploaded to PYNQ boards <em>from your local machine</em> via RPC.</p>
<p>First, install TVM with LLVM enabled. Follow the instructions <a href="https://docs.tvm.ai/install/from_source.html">here</a>.</p>
<p>Install the necessary python dependencies:</p>
<pre><code>pip3 install --user numpy decorator attrs
</code></pre>

<p>Next, you need to add a configuration file for VTA:</p>
<pre><code>cd &lt;tvm root&gt;
cp vta/config/pynq_sample.json vta/config/vta_config.json
</code></pre>

<p>When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to
get the best knob parameters to achieve fast performance. Normally, for a particular network, this log file
is generated using TVM's autotuning tool (<code>autotune.py</code>).</p>
<p>However, since this tool seems to be broken, log file
for <code>resnet18_v1_split</code> was manually created.</p>
<p>Move this log file to where the compiler can find it:</p>
<pre><code>cd &lt;project root&gt;
cp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log
</code></pre>

<h2 id="usage">Usage</h2>
<p>After setup has been complete on both the PYNQ and host end, you are
now ready to deploy the split networks. <code>demo.py</code> is a minimal example that shows you how to do this.</p>
<p>First, install additional Python dependencies:</p>
<pre><code>pip3 install --user mxnet pillow
</code></pre>

<p>Then run the demo:</p>
<pre><code>python3 demo.py [--cpu] [--nonsplit] [--i]
</code></pre>

<h3 id="options">Options:</h3>
<ul>
<li>
<p><code>--cpu</code> Run model on local machine instead of PYNQ boards.</p>
</li>
<li>
<p><code>--nonsplit</code> Run the non-split version of the model.</p>
</li>
<li>
<p><code>--i</code> Run the interactive version of the demo. This allows you to enter paths to image files to feed to model.</p>
</li>
</ul>
<p>By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays. </p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../asplos2018/real-time/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../asplos2018/real-time/" class="btn btn-xs btn-link">
        ASPLOS 2018
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../adhoc/adhoc/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../adhoc/adhoc/" class="btn btn-xs btn-link">
        Adhoc Network
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>