{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Parallel ML Docs This organization is from Georgia Tech, HPArch .","title":"Home"},{"location":"#welcome-to-parallel-ml-docs","text":"This organization is from Georgia Tech, HPArch .","title":"Welcome to Parallel ML Docs"},{"location":"asplos2018/real-time/","text":"Introduction This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3]. Installation Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference. Single device (GPU and CPU). (This is NVidia Jetson TX2 version in our paper) Dependencies: tensorflow-gpu >= 1.5.0 Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information Multiple devices (CPU and RPC). (This is Raspberry PI 3 versions in our paper) Dependencies: tensorflow >= 1.5.0 Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt Quick Start Single device (GPU and CPU) (This is NVidia Jetson TX2 version in our paper) GPU Version Execute predict file to run model inference. python predict.py CPU Version CUDA_VISIBLE_DEVICES= python predict.py Multiple devices (CPU and RPC) (This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format. AlexNet For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d VGG16 For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py Refereces [1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"ASPLOS 2018"},{"location":"asplos2018/real-time/#introduction","text":"This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3].","title":"Introduction"},{"location":"asplos2018/real-time/#installation","text":"Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference.","title":"Installation"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu","text":"(This is NVidia Jetson TX2 version in our paper) Dependencies: tensorflow-gpu >= 1.5.0 Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information","title":"Single device (GPU and CPU)."},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc","text":"(This is Raspberry PI 3 versions in our paper) Dependencies: tensorflow >= 1.5.0 Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt","title":"Multiple devices (CPU and RPC)."},{"location":"asplos2018/real-time/#quick-start","text":"","title":"Quick Start"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu_1","text":"(This is NVidia Jetson TX2 version in our paper)","title":"Single device (GPU and CPU)"},{"location":"asplos2018/real-time/#gpu-version","text":"Execute predict file to run model inference. python predict.py","title":"GPU Version"},{"location":"asplos2018/real-time/#cpu-version","text":"CUDA_VISIBLE_DEVICES= python predict.py","title":"CPU Version"},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc_1","text":"(This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format.","title":"Multiple devices (CPU and RPC)"},{"location":"asplos2018/real-time/#alexnet","text":"For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d","title":"AlexNet"},{"location":"asplos2018/real-time/#vgg16","text":"For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py","title":"VGG16"},{"location":"asplos2018/real-time/#refereces","text":"[1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"Refereces"},{"location":"camera/picamera/","text":"Using the PiCamera Module For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12) Visualizing pictures via ssh If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Using the PiCamera"},{"location":"camera/picamera/#using-the-picamera-module","text":"For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12)","title":"Using the PiCamera Module"},{"location":"camera/picamera/#visualizing-pictures-via-ssh","text":"If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Visualizing pictures via ssh"},{"location":"camera/webcam/","text":"Webcam Video/Image Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam video/image"},{"location":"camera/webcam/#webcam-videoimage","text":"Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam Video/Image"},{"location":"fpga/split-networks/","text":"FPL19 Split Networks on FPGA Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2 PYNQ FPGA boards using TVM/VTA . Github repo is available here . Motivation Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs often require more resources than those provided by individual edge devices. The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run separately on multiple edge devices in parallel. The outputs from the split networks are then concatenated and fed through the fully connected layers to perform inference. Code Description splitnet.py contains split models built with MxNet Gluon . Only resnet18_v1_split is implemented so far. resnet18_v1_split returns a split version of mxnet.gluon.model_zoo.vision.resnet18_v1 ; initialized with random weights. demo.py demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results. autotune.py uses TVM's autotuning tool to achieve fast performance when running resnet18_v1_split on PYNQ FPGA. Currently broken. Setup PYNQ Boards To deploy the split networks, first acquire 2 PYNQ boards and set them up following instructions here . After PYNQ boards are set up, follow instructions here to launch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server: INFO:root:RPCServer: bind to 0.0.0.0:9091 The RPC server should be listening on port 9091 . Local The following instructions apply to your local machine. CNN models are developed, compiled & uploaded to PYNQ boards from your local machine via RPC. First, install TVM with LLVM enabled. Follow the instructions here . Install the necessary python dependencies: pip3 install --user numpy decorator attrs Next, you need to add a configuration file for VTA: cd <tvm root> cp vta/config/pynq_sample.json vta/config/vta_config.json When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to get the best knob parameters to achieve fast performance. Normally, for a particular network, this log file is generated using TVM's autotuning tool ( autotune.py ). However, since this tool seems to be broken, log file for resnet18_v1_split was manually created. Move this log file to where the compiler can find it: cd <project root> cp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log Usage After setup has been complete on both the PYNQ and host end, you are now ready to deploy the split networks. demo.py is a minimal example that shows you how to do this. First, install additional Python dependencies: pip3 install --user mxnet pillow Then run the demo: python3 demo.py [--cpu] [--nonsplit] [--i] Options: --cpu Run model on local machine instead of PYNQ boards. --nonsplit Run the non-split version of the model. --i Run the interactive version of the demo. This allows you to enter paths to image files to feed to model. By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.","title":"FPL19 Split Networks on FPGA"},{"location":"fpga/split-networks/#fpl19-split-networks-on-fpga","text":"Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2 PYNQ FPGA boards using TVM/VTA . Github repo is available here .","title":"FPL19 Split Networks on FPGA"},{"location":"fpga/split-networks/#motivation","text":"Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs often require more resources than those provided by individual edge devices. The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run separately on multiple edge devices in parallel. The outputs from the split networks are then concatenated and fed through the fully connected layers to perform inference.","title":"Motivation"},{"location":"fpga/split-networks/#code-description","text":"splitnet.py contains split models built with MxNet Gluon . Only resnet18_v1_split is implemented so far. resnet18_v1_split returns a split version of mxnet.gluon.model_zoo.vision.resnet18_v1 ; initialized with random weights. demo.py demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results. autotune.py uses TVM's autotuning tool to achieve fast performance when running resnet18_v1_split on PYNQ FPGA. Currently broken.","title":"Code Description"},{"location":"fpga/split-networks/#setup","text":"","title":"Setup"},{"location":"fpga/split-networks/#pynq-boards","text":"To deploy the split networks, first acquire 2 PYNQ boards and set them up following instructions here . After PYNQ boards are set up, follow instructions here to launch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server: INFO:root:RPCServer: bind to 0.0.0.0:9091 The RPC server should be listening on port 9091 .","title":"PYNQ Boards"},{"location":"fpga/split-networks/#local","text":"The following instructions apply to your local machine. CNN models are developed, compiled & uploaded to PYNQ boards from your local machine via RPC. First, install TVM with LLVM enabled. Follow the instructions here . Install the necessary python dependencies: pip3 install --user numpy decorator attrs Next, you need to add a configuration file for VTA: cd <tvm root> cp vta/config/pynq_sample.json vta/config/vta_config.json When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to get the best knob parameters to achieve fast performance. Normally, for a particular network, this log file is generated using TVM's autotuning tool ( autotune.py ). However, since this tool seems to be broken, log file for resnet18_v1_split was manually created. Move this log file to where the compiler can find it: cd <project root> cp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log","title":"Local"},{"location":"fpga/split-networks/#usage","text":"After setup has been complete on both the PYNQ and host end, you are now ready to deploy the split networks. demo.py is a minimal example that shows you how to do this. First, install additional Python dependencies: pip3 install --user mxnet pillow Then run the demo: python3 demo.py [--cpu] [--nonsplit] [--i]","title":"Usage"},{"location":"fpga/split-networks/#options","text":"--cpu Run model on local machine instead of PYNQ boards. --nonsplit Run the non-split version of the model. --i Run the interactive version of the demo. This allows you to enter paths to image files to feed to model. By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.","title":"Options:"},{"location":"getting-started/setting-up-pi/","text":"Pre-Requisite Raspberry Pi can ONLY be accessed from lab's network. Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ . Connect to Raspberry Pi Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command. Python Setup Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/","title":"Connecting to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#pre-requisite","text":"Raspberry Pi can ONLY be accessed from lab's network. Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ .","title":"Pre-Requisite"},{"location":"getting-started/setting-up-pi/#connect-to-raspberry-pi","text":"Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command.","title":"Connect to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#python-setup","text":"Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/","title":"Python Setup"},{"location":"getting-started/speaker-mic/","text":"Speaker and Mic Setup Author: Ramyad Date: 7/25/2019 List All devices: cat /proc/asound/cards List all Playback Devices aplay -l See your card and device number Note: you need to set volume alsamixer -c <card_number> Note: you can also use pacmd set-source-volume <index> <volume> List all Recording Devices arecord -l or pacmd list-sources See your card and device number Control devices with alsamixer , use F6 to select your device:wq Set your Recording and Playback Device as the Default PCM Devices, in /etc/asound.conf pcm.!default { type asym capture.pcm \"mic\" playback.pcm \"speaker\" } pcm.mic { type plug slave { pcm \"hw:<card number>,<device number>\" } } pcm.speaker { type plug slave { pcm \"hw:<card number>,<device number>\" } } Live Streaming arecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000 Testing Playback speaker-test -t wav -c 2 Test Recording Device arecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw aplay --format=S16_LE --rate=16000 out.raw Speaker and Mic Volume Control alsamixer -c \"card number\"","title":"Setting Up Speaker and Mic"},{"location":"getting-started/speaker-mic/#speaker-and-mic-setup","text":"Author: Ramyad Date: 7/25/2019 List All devices: cat /proc/asound/cards List all Playback Devices aplay -l See your card and device number Note: you need to set volume alsamixer -c <card_number> Note: you can also use pacmd set-source-volume <index> <volume> List all Recording Devices arecord -l or pacmd list-sources See your card and device number Control devices with alsamixer , use F6 to select your device:wq Set your Recording and Playback Device as the Default PCM Devices, in /etc/asound.conf pcm.!default { type asym capture.pcm \"mic\" playback.pcm \"speaker\" } pcm.mic { type plug slave { pcm \"hw:<card number>,<device number>\" } } pcm.speaker { type plug slave { pcm \"hw:<card number>,<device number>\" } } Live Streaming arecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000 Testing Playback speaker-test -t wav -c 2 Test Recording Device arecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw aplay --format=S16_LE --rate=16000 out.raw Speaker and Mic Volume Control alsamixer -c \"card number\"","title":"Speaker and Mic Setup"},{"location":"irobot/keyboard/","text":"Control with Keyboard Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Keyboard Control"},{"location":"irobot/keyboard/#control-with-keyboard","text":"Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Control with Keyboard"},{"location":"irobot/navi_lidar_voice/","text":"","title":"Navigation with Lidar/Voice"},{"location":"mapping/lidar-slam/","text":"Template - Lidar SLAM","title":"Lidar SLAM"},{"location":"mapping/lidar-slam/#template-lidar-slam","text":"","title":"Template - Lidar SLAM"},{"location":"people/people/","text":"Our Group Faculty * Hyesoon Kim Graduate Students Ramyad Hadidi Jiashen Cao Undergraduate Students Fall 2017 Jiashen Cao Matthew Woodward Spring 2018 Jiashen Cao Fall 2018 Chunjun Jia Spring 2019 Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia Summer 2019 Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"People"},{"location":"people/people/#our-group","text":"","title":"Our Group"},{"location":"people/people/#faculty","text":"","title":"Faculty"},{"location":"people/people/#hyesoon-kim","text":"","title":"* Hyesoon Kim"},{"location":"people/people/#graduate-students","text":"Ramyad Hadidi Jiashen Cao","title":"Graduate Students"},{"location":"people/people/#undergraduate-students","text":"","title":"Undergraduate Students"},{"location":"people/people/#fall-2017","text":"Jiashen Cao Matthew Woodward","title":"Fall 2017"},{"location":"people/people/#spring-2018","text":"Jiashen Cao","title":"Spring 2018"},{"location":"people/people/#fall-2018","text":"Chunjun Jia","title":"Fall 2018"},{"location":"people/people/#spring-2019","text":"Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia","title":"Spring 2019"},{"location":"people/people/#summer-2019","text":"Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"Summer 2019"},{"location":"speech/deepspeech/","text":"Deepspeech on Raspberry Pi Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package Run Deepspeech with Trained Model (use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download Making Your Own Model Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Deepspeech"},{"location":"speech/deepspeech/#deepspeech-on-raspberry-pi","text":"Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package","title":"Deepspeech on Raspberry Pi"},{"location":"speech/deepspeech/#run-deepspeech-with-trained-model","text":"(use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download","title":"Run Deepspeech with Trained Model"},{"location":"speech/deepspeech/#making-your-own-model","text":"Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Making Your Own Model"},{"location":"speech/sphinx/","text":"CMU Sphinx Authors: Ramyad, Sayuj Date: 7/25/2019 Fast Setup: git clone https://github.com/parallel-ml/sphinxSpeech2Text ./install.sh make ./decode Parallel-ml repo: https://github.com/parallel-ml/sphinxSpeech2Text I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: link Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic , which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary an language model using a tool I found online link , specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file. Example of Running in Terminal pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm Note: If you get an error such as: error while loading shared libraries: libpocketsphinx.so.3 , you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below: export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig Installation sudo apt-get install bison sudo apt-get install swig cd sphinxbase-5prealpha ./autogen.sh .configure make sudo make install export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig cd ../pocketsphinx-5prealpha ./autogen.sh .configure make sudo make install Example of Running with C Contents of decode.c gcc -o decode decode.c #include <stdlib.h> #include <stdio.h> #include <time.h> #define BILLION 1000000000.0; int main(void) { struct timespec start, end; system(\"export LD_LIBRARY_PATH=/usr/local/lib\"); system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\"); system(\"echo done recording...\"); system(\"python testfiles/noiseClean.py\"); system(\"echo done cleaning...\"); clock_gettime(CLOCK_REALTIME, &start); system(\"\\ pocketsphinx_continuous \\ -infile testfiles/filtered.wav \\ -dict dicts/8050.dic \\ -lm dicts/8050.lm \\ 2>./output/unwanted-stuff.log | tee ./output/words.txt\"); // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt system(\"echo done decoding...\"); clock_gettime(CLOCK_REALTIME, &end); double time_spent = (end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / BILLION; char *timerOutput = malloc(25); sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent); system(timerOutput); } System Mic Noise Fix Using system/USB mic has noises, to clean, here is the content of noiseClean.py: outname = 'testfiles/filtered.wav' cutOffFrequency = 400.0 # from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean def running_mean(x, windowSize): cumsum = np.cumsum(np.insert(x, 0, 0)) return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize # from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174 def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True): if sample_width == 1: dtype = np.uint8 # unsigned char elif sample_width == 2: dtype = np.int16 # signed 2-byte short else: raise ValueError(\"Only supports 8 and 16 bit audio formats.\") channels = np.fromstring(raw_bytes, dtype=dtype) if interleaved: # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data channels.shape = (n_frames, n_channels) channels = channels.T else: # channels are not interleaved. All samples from channel M occur before all samples from channel M-1 channels.shape = (n_channels, n_frames) return channels with contextlib.closing(wave.open(fname,'rb')) as spf: sampleRate = spf.getframerate() ampWidth = spf.getsampwidth() nChannels = spf.getnchannels() nFrames = spf.getnframes() # Extract Raw Audio from multi-channel Wav File signal = spf.readframes(nFrames*nChannels) spf.close() channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True) # get window size # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter freqRatio = (cutOffFrequency/sampleRate) N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio) # Use moviung average (only on first channel) filtered = running_mean(channels[0], N).astype(channels.dtype) wav_file = wave.open(outname, \"w\") wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname())) wav_file.writeframes(filtered.tobytes('C')) wav_file.close()","title":"Sphinx"},{"location":"speech/sphinx/#cmu-sphinx","text":"Authors: Ramyad, Sayuj Date: 7/25/2019 Fast Setup: git clone https://github.com/parallel-ml/sphinxSpeech2Text ./install.sh make ./decode Parallel-ml repo: https://github.com/parallel-ml/sphinxSpeech2Text I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: link Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic , which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary an language model using a tool I found online link , specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file.","title":"CMU Sphinx"},{"location":"speech/sphinx/#example-of-running-in-terminal","text":"pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm Note: If you get an error such as: error while loading shared libraries: libpocketsphinx.so.3 , you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below: export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig","title":"Example of Running in Terminal"},{"location":"speech/sphinx/#installation","text":"sudo apt-get install bison sudo apt-get install swig cd sphinxbase-5prealpha ./autogen.sh .configure make sudo make install export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig cd ../pocketsphinx-5prealpha ./autogen.sh .configure make sudo make install","title":"Installation"},{"location":"speech/sphinx/#example-of-running-with-c","text":"Contents of decode.c gcc -o decode decode.c #include <stdlib.h> #include <stdio.h> #include <time.h> #define BILLION 1000000000.0; int main(void) { struct timespec start, end; system(\"export LD_LIBRARY_PATH=/usr/local/lib\"); system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\"); system(\"echo done recording...\"); system(\"python testfiles/noiseClean.py\"); system(\"echo done cleaning...\"); clock_gettime(CLOCK_REALTIME, &start); system(\"\\ pocketsphinx_continuous \\ -infile testfiles/filtered.wav \\ -dict dicts/8050.dic \\ -lm dicts/8050.lm \\ 2>./output/unwanted-stuff.log | tee ./output/words.txt\"); // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt system(\"echo done decoding...\"); clock_gettime(CLOCK_REALTIME, &end); double time_spent = (end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / BILLION; char *timerOutput = malloc(25); sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent); system(timerOutput); }","title":"Example of Running with C"},{"location":"speech/sphinx/#system-mic-noise-fix","text":"Using system/USB mic has noises, to clean, here is the content of noiseClean.py: outname = 'testfiles/filtered.wav' cutOffFrequency = 400.0 # from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean def running_mean(x, windowSize): cumsum = np.cumsum(np.insert(x, 0, 0)) return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize # from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174 def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True): if sample_width == 1: dtype = np.uint8 # unsigned char elif sample_width == 2: dtype = np.int16 # signed 2-byte short else: raise ValueError(\"Only supports 8 and 16 bit audio formats.\") channels = np.fromstring(raw_bytes, dtype=dtype) if interleaved: # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data channels.shape = (n_frames, n_channels) channels = channels.T else: # channels are not interleaved. All samples from channel M occur before all samples from channel M-1 channels.shape = (n_channels, n_frames) return channels with contextlib.closing(wave.open(fname,'rb')) as spf: sampleRate = spf.getframerate() ampWidth = spf.getsampwidth() nChannels = spf.getnchannels() nFrames = spf.getnframes() # Extract Raw Audio from multi-channel Wav File signal = spf.readframes(nFrames*nChannels) spf.close() channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True) # get window size # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter freqRatio = (cutOffFrequency/sampleRate) N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio) # Use moviung average (only on first channel) filtered = running_mean(channels[0], N).astype(channels.dtype) wav_file = wave.open(outname, \"w\") wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname())) wav_file.writeframes(filtered.tobytes('C')) wav_file.close()","title":"System Mic Noise Fix"},{"location":"speech/text-to-speech/","text":"Template - Text to Speech","title":"Text to Speech"},{"location":"speech/text-to-speech/#template-text-to-speech","text":"","title":"Template - Text to Speech"},{"location":"vision/character/","text":"Character Recognition","title":"Character Recognition"},{"location":"vision/character/#character-recognition","text":"","title":"Character Recognition"},{"location":"visual_slam/calibration/","text":"Camera Calibration The cameras that we use will have some distortion. Calibration is the process of figuring out and undoing this distortion. The process is more involved with stereo cameras. For our purposes, we have created a calibration docker image Calibration To calibrate your camera, you can use the nimashoghi/calibration Docker image. Use the ./calibration command for mono calibration and ./calibration_stereo for stereo calibration. Monocular Calibraion Below is the docker-compose.yml file for mono calibration: version: \"3.1\" services: calibration_mono: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.113:0.0 volumes: - \"./calib/:/output\" - \"./assets/config.xml:/app/assets/config.xml\" Stereo Calibraion Below is the docker-compose.yml file for stereo calibration: version: \"3.1\" services: monocular: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.114:0.0 volumes: - \"./calib_stereo/:/output\" - \"./assets/:/app/assets/\" - \"../snapshots/output/:/app/assets/stereo_input/\" command: ./calibration_stereo /app/assets/stereo_calib.xml Other resources Camera calibration With OpenCV Camera Calibration and 3D Reconstruction","title":"Camera Calibration"},{"location":"visual_slam/calibration/#camera-calibration","text":"The cameras that we use will have some distortion. Calibration is the process of figuring out and undoing this distortion. The process is more involved with stereo cameras. For our purposes, we have created a calibration docker image","title":"Camera Calibration"},{"location":"visual_slam/calibration/#calibration","text":"To calibrate your camera, you can use the nimashoghi/calibration Docker image. Use the ./calibration command for mono calibration and ./calibration_stereo for stereo calibration.","title":"Calibration"},{"location":"visual_slam/calibration/#monocular-calibraion","text":"Below is the docker-compose.yml file for mono calibration: version: \"3.1\" services: calibration_mono: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.113:0.0 volumes: - \"./calib/:/output\" - \"./assets/config.xml:/app/assets/config.xml\"","title":"Monocular Calibraion"},{"location":"visual_slam/calibration/#stereo-calibraion","text":"Below is the docker-compose.yml file for stereo calibration: version: \"3.1\" services: monocular: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.114:0.0 volumes: - \"./calib_stereo/:/output\" - \"./assets/:/app/assets/\" - \"../snapshots/output/:/app/assets/stereo_input/\" command: ./calibration_stereo /app/assets/stereo_calib.xml","title":"Stereo Calibraion"},{"location":"visual_slam/calibration/#other-resources","text":"Camera calibration With OpenCV Camera Calibration and 3D Reconstruction","title":"Other resources"},{"location":"visual_slam/installing-docker-on-pi/","text":"Installing Docker on the Raspberry Pi This document goes through the installation prcess for docker and coker-compose on the Raspberry Pi. Preparing Enironment Run the following command in your terminal: sudo apt-get install -y build-essential libssl-dev libffi-dev Installing Docker Run the following command in your terminal: curl -sSL https://get.docker.com | sudo sh && sudo usermod -aG docker pi Installing docker-compose Run the following command in your terminal: sudo apt-get -y install python-setuptools && sudo easy_install pip && sudo pip install docker-compose Other resources https://howchoo.com/g/nmrlzmq1ymn/how-to-install-docker-on-your-raspberry-pi https://stevenbreuls.com/2019/01/install-docker-on-raspberry-pi/","title":"Installing Docker on the Raspberry Pi"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker-on-the-raspberry-pi","text":"This document goes through the installation prcess for docker and coker-compose on the Raspberry Pi.","title":"Installing Docker on the Raspberry Pi"},{"location":"visual_slam/installing-docker-on-pi/#preparing-enironment","text":"Run the following command in your terminal: sudo apt-get install -y build-essential libssl-dev libffi-dev","title":"Preparing Enironment"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker","text":"Run the following command in your terminal: curl -sSL https://get.docker.com | sudo sh && sudo usermod -aG docker pi","title":"Installing Docker"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker-compose","text":"Run the following command in your terminal: sudo apt-get -y install python-setuptools && sudo easy_install pip && sudo pip install docker-compose","title":"Installing docker-compose"},{"location":"visual_slam/installing-docker-on-pi/#other-resources","text":"https://howchoo.com/g/nmrlzmq1ymn/how-to-install-docker-on-your-raspberry-pi https://stevenbreuls.com/2019/01/install-docker-on-raspberry-pi/","title":"Other resources"},{"location":"visual_slam/live-streaming-pi-video/","text":"Live Stream Raspberry Pi Video Across the Network We have built, containerized, and uploaded the mjpeg-streamer program into the nimashoghi/streamer image. You can use this image to stream a mjpeg stream over HTTP. Using a USB Camera Run the streamer image with the input_uvc input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i 'input_uvc.so --device /dev/video0' -o output_http.so Make sure to change /dev/video0 to reflect the Linux device file path. Settings for the input_uvc plugin are shown here. Stereo Cameras For stereo cameras, you can run two separate instances of the nimashoghi/streamer image. Example docker-compose configuration is shown below: version: \"3.1\" services: video0: image: nimashoghi/streamer privileged: true ports: - \"8080:8080\" command: -i 'input_uvc.so --device /dev/video0' -o output_http.so video1: image: nimashoghi/streamer privileged: true ports: - \"8081:8080\" command: -i 'input_uvc.so --device /dev/video1' -o output_http.so Using the Pi Camera Module Run the streamer image with the input_raspicam input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i input_raspicam.so -o output_http.so Settings for the input_raspicam plugin are shown here. View the Stream To view the stream, visit the URL http://{RASPBERRY_PI_LOCAL_IP}:8080/?action=stream in your browser. You can also open this network stream with VLC to record the stream as a .avi file.","title":"Live Streaming Raspberry Pi Video"},{"location":"visual_slam/live-streaming-pi-video/#live-stream-raspberry-pi-video-across-the-network","text":"We have built, containerized, and uploaded the mjpeg-streamer program into the nimashoghi/streamer image. You can use this image to stream a mjpeg stream over HTTP.","title":"Live Stream Raspberry Pi Video Across the Network"},{"location":"visual_slam/live-streaming-pi-video/#using-a-usb-camera","text":"Run the streamer image with the input_uvc input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i 'input_uvc.so --device /dev/video0' -o output_http.so Make sure to change /dev/video0 to reflect the Linux device file path. Settings for the input_uvc plugin are shown here.","title":"Using a USB Camera"},{"location":"visual_slam/live-streaming-pi-video/#stereo-cameras","text":"For stereo cameras, you can run two separate instances of the nimashoghi/streamer image. Example docker-compose configuration is shown below: version: \"3.1\" services: video0: image: nimashoghi/streamer privileged: true ports: - \"8080:8080\" command: -i 'input_uvc.so --device /dev/video0' -o output_http.so video1: image: nimashoghi/streamer privileged: true ports: - \"8081:8080\" command: -i 'input_uvc.so --device /dev/video1' -o output_http.so","title":"Stereo Cameras"},{"location":"visual_slam/live-streaming-pi-video/#using-the-pi-camera-module","text":"Run the streamer image with the input_raspicam input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i input_raspicam.so -o output_http.so Settings for the input_raspicam plugin are shown here.","title":"Using the Pi Camera Module"},{"location":"visual_slam/live-streaming-pi-video/#view-the-stream","text":"To view the stream, visit the URL http://{RASPBERRY_PI_LOCAL_IP}:8080/?action=stream in your browser. You can also open this network stream with VLC to record the stream as a .avi file.","title":"View the Stream"},{"location":"visual_slam/running-slam/","text":"Running Live SLAM To run live SLAM, you need the nimashoghi/ubuntu-xenial-orb-slam2-apps iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps image. The command given to the orb-slam2-apps image follows the following structure: <operation> <vocabulary file> [...args] Operation should be set to slam_app for offline SLAM and slam_app_live for live SLAM. The vocabulary file parameter should be set to /root/ORBvoc.txt . The rest of the arguments depend on the operation. slam_app Operation Usage: slam_app <vocabulary> [<name> <type=mono|stereo|both> <settings_mono> <settings_stereo> <left_image_folder> <right_image_folder> <times_file>]... slam_app_live Operation Usage: slam_app_live <vocabulary> [<name> <settings> <left_image_url> <right_image_url>] Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead. Running Live SLAM with MJPEG Video Input To run live SLAM with MJPEG video input, you need to do the following: - Set the operation parameter to slam_app_live . - Feed the video stream URL into the image input parameter. Example docker-compose.yml is shown below: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&param=.mjpg http://192.168.0.116:8081/?action=stream&param=.mjpg\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\" Running Live SLAM with Linux Video Device (/dev/videoN) Input Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are: - The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail). - The input video streams must be integers that indicate the number of the video input. For example, if your v Example docker-compose.yml is shown below. This example runs the livestream using the /dev/video0 and /dev/video1 devices as the left and right video inputs, respectively: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\" Running Offline SLAM To run offline SLAM, you use the slam_app operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a times.txt file which maps each image to a specific timestamp. Example docker-compose.yml for offline SLAM: version: \"3.1\" services: slam: image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest command: \"slam_app /root/ORBvoc.txt MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt\" privileged: true volumes: - \"./output/:/output/\" Monocular SLAM vs. Stereo SLAM The type parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty. Output After running offline or online SLAM, the program will output its predicted trajectory to the following file /output/{name}_keyframe_{stereo|mono}.csv . Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). View the following link for more information.","title":"Running SLAM"},{"location":"visual_slam/running-slam/#running-live-slam","text":"To run live SLAM, you need the nimashoghi/ubuntu-xenial-orb-slam2-apps iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps image. The command given to the orb-slam2-apps image follows the following structure: <operation> <vocabulary file> [...args] Operation should be set to slam_app for offline SLAM and slam_app_live for live SLAM. The vocabulary file parameter should be set to /root/ORBvoc.txt . The rest of the arguments depend on the operation.","title":"Running Live SLAM"},{"location":"visual_slam/running-slam/#slam_app-operation","text":"Usage: slam_app <vocabulary> [<name> <type=mono|stereo|both> <settings_mono> <settings_stereo> <left_image_folder> <right_image_folder> <times_file>]...","title":"slam_app Operation"},{"location":"visual_slam/running-slam/#slam_app_live-operation","text":"Usage: slam_app_live <vocabulary> [<name> <settings> <left_image_url> <right_image_url>] Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead.","title":"slam_app_live Operation"},{"location":"visual_slam/running-slam/#running-live-slam-with-mjpeg-video-input","text":"To run live SLAM with MJPEG video input, you need to do the following: - Set the operation parameter to slam_app_live . - Feed the video stream URL into the image input parameter. Example docker-compose.yml is shown below: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&param=.mjpg http://192.168.0.116:8081/?action=stream&param=.mjpg\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\"","title":"Running Live SLAM with MJPEG Video Input"},{"location":"visual_slam/running-slam/#running-live-slam-with-linux-video-device-devvideon-input","text":"Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are: - The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail). - The input video streams must be integers that indicate the number of the video input. For example, if your v Example docker-compose.yml is shown below. This example runs the livestream using the /dev/video0 and /dev/video1 devices as the left and right video inputs, respectively: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\"","title":"Running Live SLAM with Linux Video Device (/dev/videoN) Input"},{"location":"visual_slam/running-slam/#running-offline-slam","text":"To run offline SLAM, you use the slam_app operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a times.txt file which maps each image to a specific timestamp. Example docker-compose.yml for offline SLAM: version: \"3.1\" services: slam: image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest command: \"slam_app /root/ORBvoc.txt MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt\" privileged: true volumes: - \"./output/:/output/\"","title":"Running Offline SLAM"},{"location":"visual_slam/running-slam/#monocular-slam-vs-stereo-slam","text":"The type parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty.","title":"Monocular SLAM vs. Stereo SLAM"},{"location":"visual_slam/running-slam/#output","text":"After running offline or online SLAM, the program will output its predicted trajectory to the following file /output/{name}_keyframe_{stereo|mono}.csv . Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). View the following link for more information.","title":"Output"},{"location":"visual_slam/webcam-information/","text":"Webcam Information Below is a list of cameras we tried out. Logitech C270 Webcam Amazon Link: https://www.amazon.com/Logitech-Desktop-Widescreen-Calling-Recording/dp/B004FHO5Y6/ref=sr_1_3?keywords=Logitech+C270+Webcam&qid=1564694596&s=electronics&sr=1-3 - Resolution: 1280x720 - Pixel Size: 2.8um - Sensor Size: 3.58x2.02mm - Stock lens focal length: 4.2mm Minoru 3D Webcam (Red/Chrome) Amazon Link: https://www.amazon.com/gp/product/B001NXDGFY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Specifications: https://www.minoru3d.com/ VGA CMOS Sensor Maximum resolution 800\u00d7600 Camera baseline length 6 cm Maximum frame rate 30 fps L/R shutters don\u2019t synchronize(Max deviation 16.5 ms) Manual focus(from 10 cm to infinity) field angle 42 degrees (actual measurement)","title":"Webcam Information"},{"location":"visual_slam/webcam-information/#webcam-information","text":"Below is a list of cameras we tried out.","title":"Webcam Information"},{"location":"visual_slam/webcam-information/#logitech-c270-webcam","text":"Amazon Link: https://www.amazon.com/Logitech-Desktop-Widescreen-Calling-Recording/dp/B004FHO5Y6/ref=sr_1_3?keywords=Logitech+C270+Webcam&qid=1564694596&s=electronics&sr=1-3 - Resolution: 1280x720 - Pixel Size: 2.8um - Sensor Size: 3.58x2.02mm - Stock lens focal length: 4.2mm","title":"Logitech C270 Webcam"},{"location":"visual_slam/webcam-information/#minoru-3d-webcam-redchrome","text":"Amazon Link: https://www.amazon.com/gp/product/B001NXDGFY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Specifications: https://www.minoru3d.com/ VGA CMOS Sensor Maximum resolution 800\u00d7600 Camera baseline length 6 cm Maximum frame rate 30 fps L/R shutters don\u2019t synchronize(Max deviation 16.5 ms) Manual focus(from 10 cm to infinity) field angle 42 degrees (actual measurement)","title":"Minoru 3D Webcam (Red/Chrome)"}]}