{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SARAV SARAV is an undergraduate oriented research group that focuses on systems and architecture for robotics and autonomous vehicles. The group members are mainly from Georgia Tech undergraduate students and advised by Ramyad Hadidi (PhD candidate), Jiashen Cao (MS student) and Prof. Hyesoon Kim . The group belongs to HpArch group at Georgia Tech. Please check SARAV for more info.","title":"Home"},{"location":"#sarav","text":"SARAV is an undergraduate oriented research group that focuses on systems and architecture for robotics and autonomous vehicles. The group members are mainly from Georgia Tech undergraduate students and advised by Ramyad Hadidi (PhD candidate), Jiashen Cao (MS student) and Prof. Hyesoon Kim . The group belongs to HpArch group at Georgia Tech. Please check SARAV for more info.","title":"SARAV"},{"location":"adhoc/adhoc/","text":"Creating an ad hoc node with a Raspberry Pi Create a script for setting up the interfaces on boot cd ~ && touch start-batman-adv.sh && chmod +x start-batman-adv.sh Then: sudo nano start-batman-adv.sh And write this in the file: #!/bin/bash # Tell batman-adv which interface to use sudo batctl if add wlan0 # Activates the interfaces for batman-adv sudo ifconfig wlan0 up sudo ifconfig bat0 up # bat0 is created via the first command Now, create two files in /etc/network/interfaces.d called bat0 and wlan0: cd /etc/network/interfaces.d Backup your current wlan0: sudo cp wlan0 wlan0.old Then write in the wlan0 file: sudo nano wlan0 And put this: auto wlan0 iface wlan0 inet manual mtu 1532 wireless-channel 3 wireless-essid meshPi wireless-mode ad-hoc wireless-ap 02:12:34:56:78:9A Then create and write the bat0 file: sudo nano bat0 And write: auto bat0 iface bat0 inet auto pre-up /usr/sbin/batctl if add wlan0 Now, install batman-adv: sudo apt-get install -y batctl And do the following: Have batman-adv startup automatically on boot: echo 'batman-adv' | sudo tee --append /etc/modules Prevent DHCPCD from automatically configuring wlan0: echo 'denyinterfaces wlan0' | sudo tee --append /etc/dhcpcd.conf Now, go back to home: cd And enable interfaces on boot: echo \"$(pwd)/start-batman-adv.sh\" >> ~/.bashrc Now, reboot: sudo reboot Test if it worked (you should see \u201cwlan0: active\u201d): sudo batctl if Source: https://medium.com/@tdoll/how-to-setup-a-raspberry-pi-ad-hoc-network-using-batman-adv-on-raspbian-stretch-lite-dce6eb896687","title":"Adhoc Network"},{"location":"adhoc/adhoc/#creating-an-ad-hoc-node-with-a-raspberry-pi","text":"Create a script for setting up the interfaces on boot cd ~ && touch start-batman-adv.sh && chmod +x start-batman-adv.sh Then: sudo nano start-batman-adv.sh And write this in the file: #!/bin/bash # Tell batman-adv which interface to use sudo batctl if add wlan0 # Activates the interfaces for batman-adv sudo ifconfig wlan0 up sudo ifconfig bat0 up # bat0 is created via the first command Now, create two files in /etc/network/interfaces.d called bat0 and wlan0: cd /etc/network/interfaces.d Backup your current wlan0: sudo cp wlan0 wlan0.old Then write in the wlan0 file: sudo nano wlan0 And put this: auto wlan0 iface wlan0 inet manual mtu 1532 wireless-channel 3 wireless-essid meshPi wireless-mode ad-hoc wireless-ap 02:12:34:56:78:9A Then create and write the bat0 file: sudo nano bat0 And write: auto bat0 iface bat0 inet auto pre-up /usr/sbin/batctl if add wlan0 Now, install batman-adv: sudo apt-get install -y batctl And do the following: Have batman-adv startup automatically on boot: echo 'batman-adv' | sudo tee --append /etc/modules Prevent DHCPCD from automatically configuring wlan0: echo 'denyinterfaces wlan0' | sudo tee --append /etc/dhcpcd.conf Now, go back to home: cd And enable interfaces on boot: echo \"$(pwd)/start-batman-adv.sh\" >> ~/.bashrc Now, reboot: sudo reboot Test if it worked (you should see \u201cwlan0: active\u201d): sudo batctl if Source: https://medium.com/@tdoll/how-to-setup-a-raspberry-pi-ad-hoc-network-using-batman-adv-on-raspbian-stretch-lite-dce6eb896687","title":"Creating an ad hoc node with a Raspberry Pi"},{"location":"asplos2018/real-time/","text":"Introduction This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3]. Installation Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference. Single device (GPU and CPU). (This is NVidia Jetson TX2 version in our paper) Dependencies: * tensorflow-gpu >= 1.5.0 * Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information Multiple devices (CPU and RPC). (This is Raspberry PI 3 versions in our paper) Dependencies: * tensorflow >= 1.5.0 * Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt Quick Start Single device (GPU and CPU) (This is NVidia Jetson TX2 version in our paper) GPU Version Execute predict file to run model inference. python predict.py CPU Version CUDA_VISIBLE_DEVICES= python predict.py Multiple devices (CPU and RPC) (This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format. AlexNet For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d VGG16 For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py Refereces [1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"ASPLOS 2018"},{"location":"asplos2018/real-time/#introduction","text":"This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3].","title":"Introduction"},{"location":"asplos2018/real-time/#installation","text":"Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference.","title":"Installation"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu","text":"(This is NVidia Jetson TX2 version in our paper) Dependencies: * tensorflow-gpu >= 1.5.0 * Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information","title":"Single device (GPU and CPU)."},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc","text":"(This is Raspberry PI 3 versions in our paper) Dependencies: * tensorflow >= 1.5.0 * Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt","title":"Multiple devices (CPU and RPC)."},{"location":"asplos2018/real-time/#quick-start","text":"","title":"Quick Start"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu_1","text":"(This is NVidia Jetson TX2 version in our paper)","title":"Single device (GPU and CPU)"},{"location":"asplos2018/real-time/#gpu-version","text":"Execute predict file to run model inference. python predict.py","title":"GPU Version"},{"location":"asplos2018/real-time/#cpu-version","text":"CUDA_VISIBLE_DEVICES= python predict.py","title":"CPU Version"},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc_1","text":"(This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format.","title":"Multiple devices (CPU and RPC)"},{"location":"asplos2018/real-time/#alexnet","text":"For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d","title":"AlexNet"},{"location":"asplos2018/real-time/#vgg16","text":"For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py","title":"VGG16"},{"location":"asplos2018/real-time/#refereces","text":"[1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"Refereces"},{"location":"camera/picamera/","text":"Using the PiCamera Module For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12) Visualizing pictures via ssh If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Using the PiCamera"},{"location":"camera/picamera/#using-the-picamera-module","text":"For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12)","title":"Using the PiCamera Module"},{"location":"camera/picamera/#visualizing-pictures-via-ssh","text":"If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Visualizing pictures via ssh"},{"location":"camera/webcam/","text":"Webcam Video/Image Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam video/image"},{"location":"camera/webcam/#webcam-videoimage","text":"Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam Video/Image"},{"location":"conn-cars/encrypted-communication-implementation/","text":"Verified Communication Implementation The code for our implementation is located in the following repository: https://github.com/nimashoghi/v2v-node Getting Started Development Dependencies and Pre-requisities If you're planning on making changes to the code, make sure you have the following installed: - Node.js - Yarn - Python 3 - OpenCV If you simply want to use this, you can use our Docker images. Cloning the repository git clone https://github.com/nimashoghi/v2v-node.git cd v2v-node yarn Generating private key public key pairs First of all, you're going to need to generate private/public key pairs. If you have cloned the repository as shown above, you can run the following command to generate a key pair: yarn ts-node ./scripts/create-key-pair.ts Building Docker Images You can build the client Docker image by running chmod +x ./build.sh ./build.sh This will create the following Docker image: niamshoghi/dac-v2v:latest . You can change this name by editing build.sh . Deploying on the Server To enable messaging between the Pis, you will need to setup the MQTT broker on the server. To do this, you will need to: - Download the server's docker-compose.yml and mosquitto.conf files by running the following commands: wget https://raw.githubusercontent.com/nimashoghi/v2v-node/master/deployment/server/docker-compose.yaml wget https://raw.githubusercontent.com/nimashoghi/v2v-node/master/deployment/server/mosquitto.conf Run docker-compose up -d to start the server. Keep the IP of this server for the client deployment process. Deploying on the Pi To deploy the program on the Pi, go through the following steps: - Download the docker-compose.yml configuration onto each Pi. Modify the docker-compose.yml file so that the MQTT_HOST is the IP of the server that you deployed the MQTT broker on (from the previous step). Also modify the ALL_TOPICS variable so that it contains a comma-delimited list of the client ids for each Pi. The CLIENT_ID is a unique id that repesents the name of the MQTT channel for that Pi. - Make sure that the public and private keys for the Pi are located at the following locations, respectively: ./keys/{{CLIENT_ID}}/public.pem and ./keys/{{CLIENT_ID}}/private.pem . - Run the following command on the Pi to start the app: CLIENT_ID={{CLIENT_ID}} docker-compose up -d . - The network should add each Pi into the system as it connects. Testing Once everything is set up, you can: - Set up two Roombas, one behind the other, so that the one in the back can see the QR code of the one in the front. - Send movement commands to the front Roomba, using our modified keyboard control script . The setup instructions are the same as the iRobot keyboard control instructions on the parallel-ml docs .","title":"Verified Communication Implementation"},{"location":"conn-cars/encrypted-communication-implementation/#verified-communication-implementation","text":"The code for our implementation is located in the following repository: https://github.com/nimashoghi/v2v-node","title":"Verified Communication Implementation"},{"location":"conn-cars/encrypted-communication-implementation/#getting-started","text":"","title":"Getting Started"},{"location":"conn-cars/encrypted-communication-implementation/#development-dependencies-and-pre-requisities","text":"If you're planning on making changes to the code, make sure you have the following installed: - Node.js - Yarn - Python 3 - OpenCV If you simply want to use this, you can use our Docker images.","title":"Development Dependencies and Pre-requisities"},{"location":"conn-cars/encrypted-communication-implementation/#cloning-the-repository","text":"git clone https://github.com/nimashoghi/v2v-node.git cd v2v-node yarn","title":"Cloning the repository"},{"location":"conn-cars/encrypted-communication-implementation/#generating-private-key-public-key-pairs","text":"First of all, you're going to need to generate private/public key pairs. If you have cloned the repository as shown above, you can run the following command to generate a key pair: yarn ts-node ./scripts/create-key-pair.ts","title":"Generating private key public key pairs"},{"location":"conn-cars/encrypted-communication-implementation/#building-docker-images","text":"You can build the client Docker image by running chmod +x ./build.sh ./build.sh This will create the following Docker image: niamshoghi/dac-v2v:latest . You can change this name by editing build.sh .","title":"Building Docker Images"},{"location":"conn-cars/encrypted-communication-implementation/#deploying-on-the-server","text":"To enable messaging between the Pis, you will need to setup the MQTT broker on the server. To do this, you will need to: - Download the server's docker-compose.yml and mosquitto.conf files by running the following commands: wget https://raw.githubusercontent.com/nimashoghi/v2v-node/master/deployment/server/docker-compose.yaml wget https://raw.githubusercontent.com/nimashoghi/v2v-node/master/deployment/server/mosquitto.conf Run docker-compose up -d to start the server. Keep the IP of this server for the client deployment process.","title":"Deploying on the Server"},{"location":"conn-cars/encrypted-communication-implementation/#deploying-on-the-pi","text":"To deploy the program on the Pi, go through the following steps: - Download the docker-compose.yml configuration onto each Pi. Modify the docker-compose.yml file so that the MQTT_HOST is the IP of the server that you deployed the MQTT broker on (from the previous step). Also modify the ALL_TOPICS variable so that it contains a comma-delimited list of the client ids for each Pi. The CLIENT_ID is a unique id that repesents the name of the MQTT channel for that Pi. - Make sure that the public and private keys for the Pi are located at the following locations, respectively: ./keys/{{CLIENT_ID}}/public.pem and ./keys/{{CLIENT_ID}}/private.pem . - Run the following command on the Pi to start the app: CLIENT_ID={{CLIENT_ID}} docker-compose up -d . - The network should add each Pi into the system as it connects.","title":"Deploying on the Pi"},{"location":"conn-cars/encrypted-communication-implementation/#testing","text":"Once everything is set up, you can: - Set up two Roombas, one behind the other, so that the one in the back can see the QR code of the one in the front. - Send movement commands to the front Roomba, using our modified keyboard control script . The setup instructions are the same as the iRobot keyboard control instructions on the parallel-ml docs .","title":"Testing"},{"location":"conn-cars/encrypted-communication/","text":"Verified Communication Problem Statement and Solution Secure and Verified Inter-vehicle Communication and Performance Considerations Vehicle-to-Vehicle (V2V) Communication and Existing Work Is going to be implemented in many future cars, including those that are not self-driving. See https://www.theverge.com/2016/12/13/13936342/wireless-vehicle-to-vehicle-communication-v2v-v2i-dot-nhtsa Potentials of V2V Traffic lights could broadcast their state over the network. Vehicles could broadcast their speeds. Problems with V2V Infrastructure Currently, a separate protocol has been proposed for V2V, and not many cars support this protocol. In our experiments, we will be setting up a mesh WiFi network to communicate (as an emulation of this protocol). This may prove to be viable in the real world as well. Security Security is the biggest concern with these efforts. If our system design has clear security flaws, then bad actors can abuse these flaws to do some massive harm. Some key security concerns are listed below. 1. Impersonation and Falsification A bad actor could impersonate another vehicle and report information that is incorrect, leading to accidents. 2. Lack of Verification When you get a message, you should be able to verify which vehicle, in the physical world, that message comes from. Many existing systems do not implement this (?) 3. Duplication of Legitimate Messages Due to the peer-to-peer nature of the network setup, nearby vehicles will receive all messages, even those that are not particularly interesting to them. Assuming that some underlying digital signing mechanism (using asymmetric encryption) is used, there is still a problem in cases where. 4. Privacy Broadcasting information to vehicles near you has some clear privacy concerns. Our Solution: Verified and Encrypted Messaging Between Vehicles Every vehicle (and other broadcasting entities in the street, such as traffic lights) has a public/private key pair. The vehicle broadcasts its public key using a QR code (or some other visual method of transmitting information). When a vehicle wants to transmit information to other vehicles nearby, it encrypts that message with its private key. Other vehicles can verify that the message is coming from the right vehicle by decrypting that message using the QR code for that vehicle. This way, you can ensure that the message is coming from the correct source. This solution solves problems (1) and (2). Time-Dependence To prevent the duplication of signed messages, we can encode the current time into the sent packet. Then, clients will only trust messages within a few seconds of their transmission time. Acknowledgement Packets The final security guarantee, added on top of the existing security measures, will be an end-to-end encrypted acknowledgment process. When a vehicle broadcasts some sort of update packet (e.g. speed update), other clients will receive this message and verify it using the vehicle's QR code. Once this verification is successful, clients can send a direct message to the source vehicle, establish an encrypted channel of communication with them, and ask them to verify that the packet received is legitimate and valid in the current time frame. Multiple \"Hops\" In cases where vehicles do not have a direct line of sight to the QR codes of the subjects they want to verify, we implement a \"hop\" system. In this case, every vehicle that verifies some message. Imagine the following scenario: In the scenario below, imagine if four cars are all in the same lane consecutively: A B C D (A is in front of B who is in front of C who is in front of D), and A wants to verifiably transmit some message to D. Vehicle A sends a speed update to 200 mph. Vehicle B, who has a direct line of sight with A's QR code, receives vehicle A's speed update packet and verifies this information by decrypting with vehicle A's public key. Vehicle B then broadcasts that it has verified vehicle A's speed update to 200 mph. Vehicle C receives vehicle A's original speed update packet but is unable to verify it. Vehicle C receives vehicle B's verification packet and verifies its authenticity by decrypting with vehicle B's public key. Vehicle C then broadcasts that it has verified vehicle B's verification of vehicle A's speed up. Vehicle D receives vehicle A's original speed update packet but is unable to verify it. Vehicle D receives vehicle C's verification packet of vehicle B, which pertains to the original speed up packet of vehicle A, and verifies its authenticity by decrypting with vehicle C's public key. Vehicle D has now received the original information from vehicle A. Please note that the \"trustability\" of the message drops for every hop that the packet takes. Please see the next section for more details Trustability As packets hop through vehicle to vehicle, we calculate a trustability factor for them. For the sake of simplicity, we define the \"trustability\" of a packet to be 1 / (# of hops that the packet takes). For example, the packet from the previous section took 3 hops from A to D, so it has a trustability factor of 1/3 or 33%. A vehicle will only trust a packet if the net trustability factor is 100%. For example, if D receives 2 other verification messages about A's speed up, then it will trust the event. This may happen if other vehicles in other lanes also send verification messages. Scan a QR code every 500 ms If we scan a QR code and validate it, then we cache for some time period. You also want to mark this into your localization. We could expand the integration with the localization system and exploring the security risks of doing so. If we get some message from a vehicle that we don't recognize (i.e. we haven't scanned the QR code for this message yet), then we wait for some time period before fully discarding the message.","title":"Verified Communication Problem Statement and Solution"},{"location":"conn-cars/encrypted-communication/#verified-communication-problem-statement-and-solution","text":"","title":"Verified Communication Problem Statement and Solution"},{"location":"conn-cars/encrypted-communication/#secure-and-verified-inter-vehicle-communication-and-performance-considerations","text":"","title":"Secure and Verified Inter-vehicle Communication and Performance Considerations"},{"location":"conn-cars/encrypted-communication/#vehicle-to-vehicle-v2v-communication-and-existing-work","text":"Is going to be implemented in many future cars, including those that are not self-driving. See https://www.theverge.com/2016/12/13/13936342/wireless-vehicle-to-vehicle-communication-v2v-v2i-dot-nhtsa","title":"Vehicle-to-Vehicle (V2V) Communication and Existing Work"},{"location":"conn-cars/encrypted-communication/#potentials-of-v2v","text":"Traffic lights could broadcast their state over the network. Vehicles could broadcast their speeds.","title":"Potentials of V2V"},{"location":"conn-cars/encrypted-communication/#problems-with-v2v","text":"","title":"Problems with V2V"},{"location":"conn-cars/encrypted-communication/#infrastructure","text":"Currently, a separate protocol has been proposed for V2V, and not many cars support this protocol. In our experiments, we will be setting up a mesh WiFi network to communicate (as an emulation of this protocol). This may prove to be viable in the real world as well.","title":"Infrastructure"},{"location":"conn-cars/encrypted-communication/#security","text":"Security is the biggest concern with these efforts. If our system design has clear security flaws, then bad actors can abuse these flaws to do some massive harm. Some key security concerns are listed below.","title":"Security"},{"location":"conn-cars/encrypted-communication/#1-impersonation-and-falsification","text":"A bad actor could impersonate another vehicle and report information that is incorrect, leading to accidents.","title":"1. Impersonation and Falsification"},{"location":"conn-cars/encrypted-communication/#2-lack-of-verification","text":"When you get a message, you should be able to verify which vehicle, in the physical world, that message comes from. Many existing systems do not implement this (?)","title":"2. Lack of Verification"},{"location":"conn-cars/encrypted-communication/#3-duplication-of-legitimate-messages","text":"Due to the peer-to-peer nature of the network setup, nearby vehicles will receive all messages, even those that are not particularly interesting to them. Assuming that some underlying digital signing mechanism (using asymmetric encryption) is used, there is still a problem in cases where.","title":"3. Duplication of Legitimate Messages"},{"location":"conn-cars/encrypted-communication/#4-privacy","text":"Broadcasting information to vehicles near you has some clear privacy concerns.","title":"4. Privacy"},{"location":"conn-cars/encrypted-communication/#our-solution-verified-and-encrypted-messaging-between-vehicles","text":"Every vehicle (and other broadcasting entities in the street, such as traffic lights) has a public/private key pair. The vehicle broadcasts its public key using a QR code (or some other visual method of transmitting information). When a vehicle wants to transmit information to other vehicles nearby, it encrypts that message with its private key. Other vehicles can verify that the message is coming from the right vehicle by decrypting that message using the QR code for that vehicle. This way, you can ensure that the message is coming from the correct source. This solution solves problems (1) and (2).","title":"Our Solution: Verified and Encrypted Messaging Between Vehicles"},{"location":"conn-cars/encrypted-communication/#time-dependence","text":"To prevent the duplication of signed messages, we can encode the current time into the sent packet. Then, clients will only trust messages within a few seconds of their transmission time.","title":"Time-Dependence"},{"location":"conn-cars/encrypted-communication/#acknowledgement-packets","text":"The final security guarantee, added on top of the existing security measures, will be an end-to-end encrypted acknowledgment process. When a vehicle broadcasts some sort of update packet (e.g. speed update), other clients will receive this message and verify it using the vehicle's QR code. Once this verification is successful, clients can send a direct message to the source vehicle, establish an encrypted channel of communication with them, and ask them to verify that the packet received is legitimate and valid in the current time frame.","title":"Acknowledgement Packets"},{"location":"conn-cars/encrypted-communication/#multiple-hops","text":"In cases where vehicles do not have a direct line of sight to the QR codes of the subjects they want to verify, we implement a \"hop\" system. In this case, every vehicle that verifies some message. Imagine the following scenario: In the scenario below, imagine if four cars are all in the same lane consecutively: A B C D (A is in front of B who is in front of C who is in front of D), and A wants to verifiably transmit some message to D. Vehicle A sends a speed update to 200 mph. Vehicle B, who has a direct line of sight with A's QR code, receives vehicle A's speed update packet and verifies this information by decrypting with vehicle A's public key. Vehicle B then broadcasts that it has verified vehicle A's speed update to 200 mph. Vehicle C receives vehicle A's original speed update packet but is unable to verify it. Vehicle C receives vehicle B's verification packet and verifies its authenticity by decrypting with vehicle B's public key. Vehicle C then broadcasts that it has verified vehicle B's verification of vehicle A's speed up. Vehicle D receives vehicle A's original speed update packet but is unable to verify it. Vehicle D receives vehicle C's verification packet of vehicle B, which pertains to the original speed up packet of vehicle A, and verifies its authenticity by decrypting with vehicle C's public key. Vehicle D has now received the original information from vehicle A. Please note that the \"trustability\" of the message drops for every hop that the packet takes. Please see the next section for more details","title":"Multiple \"Hops\""},{"location":"conn-cars/encrypted-communication/#trustability","text":"As packets hop through vehicle to vehicle, we calculate a trustability factor for them. For the sake of simplicity, we define the \"trustability\" of a packet to be 1 / (# of hops that the packet takes). For example, the packet from the previous section took 3 hops from A to D, so it has a trustability factor of 1/3 or 33%. A vehicle will only trust a packet if the net trustability factor is 100%. For example, if D receives 2 other verification messages about A's speed up, then it will trust the event. This may happen if other vehicles in other lanes also send verification messages.","title":"Trustability"},{"location":"conn-cars/encrypted-communication/#scan-a-qr-code-every-500-ms","text":"If we scan a QR code and validate it, then we cache for some time period. You also want to mark this into your localization. We could expand the integration with the localization system and exploring the security risks of doing so. If we get some message from a vehicle that we don't recognize (i.e. we haven't scanned the QR code for this message yet), then we wait for some time period before fully discarding the message.","title":"Scan a QR code every 500 ms"},{"location":"conn-cars/mqtt/","text":"Setting Up MQTT-Based Communication Between Pis We use MQTT to set up a single-publisher, multiple-subscriber event system where: - All Raspberry Pis connect to the same MQTT broker. - Each Raspberry Pi can broadcast a message to other Raspberry Pis over the MQTT network Code The code is located at the following GitHub repository: https://github.com/parallel-ml/robot-communication Getting Started Client On the client, run python app.py --broker \"mqtt://{IP_FOR_BROKER}\" --label \"{LABEL}\" --topics-publish {PUBLISH TOPICS} --topics-subscribe {SUBSCRIBE TOPICS} to start the Python test client. Broker Clone the repository and run docker-compose up -d to start the MQTT server.","title":"Setting Up MQTT-Based Communication Between Pis"},{"location":"conn-cars/mqtt/#setting-up-mqtt-based-communication-between-pis","text":"We use MQTT to set up a single-publisher, multiple-subscriber event system where: - All Raspberry Pis connect to the same MQTT broker. - Each Raspberry Pi can broadcast a message to other Raspberry Pis over the MQTT network","title":"Setting Up MQTT-Based Communication Between Pis"},{"location":"conn-cars/mqtt/#code","text":"The code is located at the following GitHub repository: https://github.com/parallel-ml/robot-communication","title":"Code"},{"location":"conn-cars/mqtt/#getting-started","text":"","title":"Getting Started"},{"location":"conn-cars/mqtt/#client","text":"On the client, run python app.py --broker \"mqtt://{IP_FOR_BROKER}\" --label \"{LABEL}\" --topics-publish {PUBLISH TOPICS} --topics-subscribe {SUBSCRIBE TOPICS} to start the Python test client.","title":"Client"},{"location":"conn-cars/mqtt/#broker","text":"Clone the repository and run docker-compose up -d to start the MQTT server.","title":"Broker"},{"location":"contr_docs/contr_docs/","text":"Contribute to Docs Documentations are in markdown format. All changes should be made under /docs directory and commit to master branch before deployment. If you add a new file, make sure to to change mkdocs.yml Publish Docs (user) You need to fork the repo , update locally, and submit a pull request. You can test your local repo with (for more info check mkdocs ) mkdocs serve Publish Docs (admin) After commit all changes to master branch, run the command mkdocs gh-deploy The mkdocs library will take care rest of the work and publish the docs under Github Pages .","title":"Contributing to Docs"},{"location":"contr_docs/contr_docs/#contribute-to-docs","text":"Documentations are in markdown format. All changes should be made under /docs directory and commit to master branch before deployment. If you add a new file, make sure to to change mkdocs.yml","title":"Contribute to Docs"},{"location":"contr_docs/contr_docs/#publish-docs-user","text":"You need to fork the repo , update locally, and submit a pull request. You can test your local repo with (for more info check mkdocs ) mkdocs serve","title":"Publish Docs (user)"},{"location":"contr_docs/contr_docs/#publish-docs-admin","text":"After commit all changes to master branch, run the command mkdocs gh-deploy The mkdocs library will take care rest of the work and publish the docs under Github Pages .","title":"Publish Docs (admin)"},{"location":"drone/accessing-navio/","text":"Accessing Navio and Executing code First connect to the NAVIO+PI $ ssh pi@192.168.1.48 Password : raspberry You should now have SSHed into the Navio+Pi Note, the drone has already been configured to connect to the lab wifi network. Also, the router assigns the same IP 192.168.1.48 to the drone every time. Installing VENV and APIs pip install dronekit sudo emlidtool --on_boot=True pip freeze | grep dronekit Verify installation and version Running DroneKit and other code Please visit DroneKit docs on github to see what each function does. You can code in Python 3 or C++ To run a Python file, python test.py To run a C++ file, first compile and then ./test If everyhting is set up correctly, the drone will execute the commands given to it Monitoring with MissionPlanner Start up MissionPlanner Connect the other TELEM USB module to your computer Select COM3 from top-right drop-down menu Click Connect Make sure drone is powered MissionPlanner should automatically connect to the drone and download all the variables and their types and start real-time monitoring","title":"Accessing Navio"},{"location":"drone/accessing-navio/#accessing-navio-and-executing-code","text":"","title":"Accessing Navio and Executing code"},{"location":"drone/accessing-navio/#first-connect-to-the-naviopi","text":"$ ssh pi@192.168.1.48 Password : raspberry You should now have SSHed into the Navio+Pi Note, the drone has already been configured to connect to the lab wifi network. Also, the router assigns the same IP 192.168.1.48 to the drone every time.","title":"First connect to the NAVIO+PI"},{"location":"drone/accessing-navio/#installing-venv-and-apis","text":"pip install dronekit sudo emlidtool --on_boot=True pip freeze | grep dronekit Verify installation and version","title":"Installing VENV and APIs"},{"location":"drone/accessing-navio/#running-dronekit-and-other-code","text":"Please visit DroneKit docs on github to see what each function does. You can code in Python 3 or C++ To run a Python file, python test.py To run a C++ file, first compile and then ./test If everyhting is set up correctly, the drone will execute the commands given to it","title":"Running DroneKit and other code"},{"location":"drone/accessing-navio/#monitoring-with-missionplanner","text":"Start up MissionPlanner Connect the other TELEM USB module to your computer Select COM3 from top-right drop-down menu Click Connect Make sure drone is powered MissionPlanner should automatically connect to the drone and download all the variables and their types and start real-time monitoring","title":"Monitoring with MissionPlanner"},{"location":"drone/building-drone/","text":"Building And Configuring The Raspberry Pi Drone Requirements Hardware: Parts List Router 32 GB microSD card Software: Mission Planner [Needs Windows system or virtual machine] Python 3 Etcher [For Windows ] Or Belina Etcher [For macOS ] Introduction To build the drone, we need to gather several hardware components. Please refer to the Parts List and ensure all parts are present before building. Build Guide Note : Most instructions can be found in the manual located in the frame box. First assemble the PI + NAVIO. Plug in the HAT into the GPIO pins on the RPI. Solder the bullet connectors onto the motor connections Solder the battery connector onto the Power Distribution Board (PDB) Screw in the legs of the frame Screw in the top plate to the frame Attach motors to the frame according to the rotational direction listed in the motor manual Use double sided tape and attach Raspberry pi + NAVIO to drone top plate Use double sided tape and attach the PPM encoder to the frame. Connect the battery connectors to the PPM encoder. Stick the RC receiver onto the frame. Connect receiver to the NAVIO Connect PPM outputs to NAVIO Use zip ties and attach ESCs to the bottom of the legs Assemble the GPS mount and zip tie it into the back-right leg (Note : GPS unit must point North-South) Attach GPS on mount and connect GPS to NAVIO Connect ESCs to motors and ESCs pwm to PPM encoder Connect battery to battery connector Finally connect TELEM module to HAT and stick module onto frame Hardware Calibration Calibrate ESCs using the RC controller and charged battery Calibrate barometer using known height Calibrate Compasses using the GPS signal Installation Download EMLID OS image from EMLID website (for Navio2) Use Etcher to burn image onto SD card Insert SD card into PI Configure the PI to connect to WIFI Download MissionPlanner on Windows machine or VM Accessing APIs and flying Please visit Accessing Navio","title":"Building the drone"},{"location":"drone/building-drone/#building-and-configuring-the-raspberry-pi-drone","text":"","title":"Building And Configuring The Raspberry Pi Drone"},{"location":"drone/building-drone/#requirements","text":"Hardware: Parts List Router 32 GB microSD card Software: Mission Planner [Needs Windows system or virtual machine] Python 3 Etcher [For Windows ] Or Belina Etcher [For macOS ]","title":"Requirements"},{"location":"drone/building-drone/#introduction","text":"To build the drone, we need to gather several hardware components. Please refer to the Parts List and ensure all parts are present before building.","title":"Introduction"},{"location":"drone/building-drone/#build-guide","text":"Note : Most instructions can be found in the manual located in the frame box. First assemble the PI + NAVIO. Plug in the HAT into the GPIO pins on the RPI. Solder the bullet connectors onto the motor connections Solder the battery connector onto the Power Distribution Board (PDB) Screw in the legs of the frame Screw in the top plate to the frame Attach motors to the frame according to the rotational direction listed in the motor manual Use double sided tape and attach Raspberry pi + NAVIO to drone top plate Use double sided tape and attach the PPM encoder to the frame. Connect the battery connectors to the PPM encoder. Stick the RC receiver onto the frame. Connect receiver to the NAVIO Connect PPM outputs to NAVIO Use zip ties and attach ESCs to the bottom of the legs Assemble the GPS mount and zip tie it into the back-right leg (Note : GPS unit must point North-South) Attach GPS on mount and connect GPS to NAVIO Connect ESCs to motors and ESCs pwm to PPM encoder Connect battery to battery connector Finally connect TELEM module to HAT and stick module onto frame","title":"Build Guide"},{"location":"drone/building-drone/#hardware-calibration","text":"Calibrate ESCs using the RC controller and charged battery Calibrate barometer using known height Calibrate Compasses using the GPS signal","title":"Hardware Calibration"},{"location":"drone/building-drone/#installation","text":"Download EMLID OS image from EMLID website (for Navio2) Use Etcher to burn image onto SD card Insert SD card into PI Configure the PI to connect to WIFI Download MissionPlanner on Windows machine or VM","title":"Installation"},{"location":"drone/building-drone/#accessing-apis-and-flying","text":"Please visit Accessing Navio","title":"Accessing APIs and flying"},{"location":"drone/parts-list/","text":"Parts List for Drone Overview For the parts, ensure that all parts are bought exactly. This is because previous drone team members from Fall 2019 have ensured that all the parts bought were fully compatible. Replacing a part with another may lead to burnt ESCs or exploding LiPo batteries. Please be careful. Drone specifications Class : 450mm (Medium-Large) Weight = (272g) + (50g) + (50g) + (26g) +(248g) = 646g = approx 1.4 lbs Motor power = 935 KV [Brushless motors] Payload capacity estimate = 200g List (with links) Raspberry Pi 3 Model B+ NAVIO KIT ESCs 4 PACK Motors 4 PACK RC Controller Frame Props Battery Battery Charger Telemetry LiPo Fire-proof Case PPM Encoder Battery Connector GPS Mount Velcro Straps Scotch Mounting Tape Zip Ties Soldering Iron Kit (lab already has) Allen Wrench Electrical Tape","title":"Parts List"},{"location":"drone/parts-list/#parts-list-for-drone","text":"","title":"Parts List for Drone"},{"location":"drone/parts-list/#overview","text":"For the parts, ensure that all parts are bought exactly. This is because previous drone team members from Fall 2019 have ensured that all the parts bought were fully compatible. Replacing a part with another may lead to burnt ESCs or exploding LiPo batteries. Please be careful.","title":"Overview"},{"location":"drone/parts-list/#drone-specifications","text":"Class : 450mm (Medium-Large) Weight = (272g) + (50g) + (50g) + (26g) +(248g) = 646g = approx 1.4 lbs Motor power = 935 KV [Brushless motors] Payload capacity estimate = 200g","title":"Drone specifications"},{"location":"drone/parts-list/#list-with-links","text":"Raspberry Pi 3 Model B+ NAVIO KIT ESCs 4 PACK Motors 4 PACK RC Controller Frame Props Battery Battery Charger Telemetry LiPo Fire-proof Case PPM Encoder Battery Connector GPS Mount Velcro Straps Scotch Mounting Tape Zip Ties Soldering Iron Kit (lab already has) Allen Wrench Electrical Tape","title":"List (with links)"},{"location":"fpga/split-networks/","text":"FPL19 Split Networks on FPGA Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2 PYNQ FPGA boards using TVM/VTA . Github repo is available here . Motivation Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs often require more resources than those provided by individual edge devices. The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run separately on multiple edge devices in parallel. The outputs from the split networks are then concatenated and fed through the fully connected layers to perform inference. Code Description splitnet.py contains split models built with MxNet Gluon . Only resnet18_v1_split is implemented so far. resnet18_v1_split returns a split version of mxnet.gluon.model_zoo.vision.resnet18_v1 ; initialized with random weights. demo.py demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results. autotune.py uses TVM's autotuning tool to achieve fast performance when running resnet18_v1_split on PYNQ FPGA. Currently broken. Setup PYNQ Boards To deploy the split networks, first acquire 2 PYNQ boards and set them up following instructions here . After PYNQ boards are set up, follow instructions here to launch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server: INFO:root:RPCServer: bind to 0.0.0.0:9091 The RPC server should be listening on port 9091 . Local The following instructions apply to your local machine. CNN models are developed, compiled & uploaded to PYNQ boards from your local machine via RPC. First, install TVM with LLVM enabled. Follow the instructions here . Install the necessary python dependencies: pip3 install --user numpy decorator attrs Next, you need to add a configuration file for VTA: cd <tvm root> cp vta/config/pynq_sample.json vta/config/vta_config.json When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to get the best knob parameters to achieve fast performance. Normally, for a particular network, this log file is generated using TVM's autotuning tool ( autotune.py ). However, since this tool seems to be broken, log file for resnet18_v1_split was manually created. Move this log file to where the compiler can find it: cd <project root> cp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log Usage After setup has been complete on both the PYNQ and host end, you are now ready to deploy the split networks. demo.py is a minimal example that shows you how to do this. First, install additional Python dependencies: pip3 install --user mxnet pillow Then run the demo: python3 demo.py [--cpu] [--nonsplit] [--i] Options: --cpu Run model on local machine instead of PYNQ boards. --nonsplit Run the non-split version of the model. --i Run the interactive version of the demo. This allows you to enter paths to image files to feed to model. By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.","title":"FPGA"},{"location":"fpga/split-networks/#fpl19-split-networks-on-fpga","text":"Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2 PYNQ FPGA boards using TVM/VTA . Github repo is available here .","title":"FPL19 Split Networks on FPGA"},{"location":"fpga/split-networks/#motivation","text":"Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs often require more resources than those provided by individual edge devices. The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run separately on multiple edge devices in parallel. The outputs from the split networks are then concatenated and fed through the fully connected layers to perform inference.","title":"Motivation"},{"location":"fpga/split-networks/#code-description","text":"splitnet.py contains split models built with MxNet Gluon . Only resnet18_v1_split is implemented so far. resnet18_v1_split returns a split version of mxnet.gluon.model_zoo.vision.resnet18_v1 ; initialized with random weights. demo.py demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results. autotune.py uses TVM's autotuning tool to achieve fast performance when running resnet18_v1_split on PYNQ FPGA. Currently broken.","title":"Code Description"},{"location":"fpga/split-networks/#setup","text":"","title":"Setup"},{"location":"fpga/split-networks/#pynq-boards","text":"To deploy the split networks, first acquire 2 PYNQ boards and set them up following instructions here . After PYNQ boards are set up, follow instructions here to launch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server: INFO:root:RPCServer: bind to 0.0.0.0:9091 The RPC server should be listening on port 9091 .","title":"PYNQ Boards"},{"location":"fpga/split-networks/#local","text":"The following instructions apply to your local machine. CNN models are developed, compiled & uploaded to PYNQ boards from your local machine via RPC. First, install TVM with LLVM enabled. Follow the instructions here . Install the necessary python dependencies: pip3 install --user numpy decorator attrs Next, you need to add a configuration file for VTA: cd <tvm root> cp vta/config/pynq_sample.json vta/config/vta_config.json When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to get the best knob parameters to achieve fast performance. Normally, for a particular network, this log file is generated using TVM's autotuning tool ( autotune.py ). However, since this tool seems to be broken, log file for resnet18_v1_split was manually created. Move this log file to where the compiler can find it: cd <project root> cp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log","title":"Local"},{"location":"fpga/split-networks/#usage","text":"After setup has been complete on both the PYNQ and host end, you are now ready to deploy the split networks. demo.py is a minimal example that shows you how to do this. First, install additional Python dependencies: pip3 install --user mxnet pillow Then run the demo: python3 demo.py [--cpu] [--nonsplit] [--i]","title":"Usage"},{"location":"fpga/split-networks/#options","text":"--cpu Run model on local machine instead of PYNQ boards. --nonsplit Run the non-split version of the model. --i Run the interactive version of the demo. This allows you to enter paths to image files to feed to model. By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.","title":"Options:"},{"location":"getting-started/setting-up-pi/","text":"Pre-Requisite Raspberry Pi can ONLY be accessed from lab's network. They are automatically configured to connect the the below WiFi Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ . Connect to Raspberry Pi Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command. Python Setup Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/ OS Install We use HyPriot OS , a lightwieght Ubuntu-based, for our PIs, unless said otherwise. It does not have a GUI, but it will always derive the HDMI output (even if you did not connect it during boot). For making an SD card, you can use their flash tool. Below is an example: flash -u cloud-config.yml https://github.com/hypriot/image-builder-rpi/releases/download/v1.11.0/hypriotos-rpi-v1.11.0.img.zip and the content for cloud-config.yml to automatically configure user, password, and WiFi connection: #cloud-config # vim: syntax=yaml # # The current version of cloud-init in the Hypriot rpi-64 is 0.7.9 # When dealing with cloud-init, it is SUPER important to know the version # I have wasted many hours creating servers to find out the module I was trying to use wasn't in the cloud-init version I had # Documentation: http://cloudinit.readthedocs.io/en/0.7.9/index.html # Set your hostname here, the manage_etc_hosts will update the hosts file entries as well hostname: pi manage_etc_hosts: true # You could modify this for your own user information users: - name: pi gecos: \"Hypriot Pirate\" sudo: ALL=(ALL) NOPASSWD:ALL shell: /bin/bash groups: users,docker,video plain_text_passwd: raspberry lock_passwd: false ssh_pwauth: true chpasswd: { expire: false } # # Set the locale of the system # locale: \"en_US.UTF-8\" # # Set the timezone # # Value of 'timezone' must exist in /usr/share/zoneinfo # timezone: \"America/Los_Angeles\" # # Update apt packages on first boot # package_update: true # package_upgrade: true # package_reboot_if_required: true package_upgrade: false # # Install any additional apt packages you need here # packages: # - ntp # # WiFi connect to HotSpot # To make wifi work with RPi3 and RPi0 # you also have to set \"enable_uart=0\" in config.txt # See no-uart-config.txt for an example. # # # - use `wpa_passphrase SSID PASSWORD` to encrypt the psk write_files: - content: | allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf iface default inet dhcp path: /etc/network/interfaces.d/wlan0 - content: | country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"NETGEAR79\" psk=\"78zBJr!4bVdpaFIQ\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN } path: /etc/wpa_supplicant/wpa_supplicant.conf # These commands will be ran once on first boot only runcmd: # Pickup the hostname changes - 'systemctl restart avahi-daemon' # Activate WiFi interface - 'ifup wlan0' Connect to Wifi with Terminal edit the file /etc/wpa_supplicant/wpa_supplicant.conf with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf . Copy below info: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=US network={ ssid=\"NETGEAR79\" psk=\"78zBJr!4bVdpaFIQ\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN } Exit with ctrl^x, Enter.","title":"Connecting to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#pre-requisite","text":"Raspberry Pi can ONLY be accessed from lab's network. They are automatically configured to connect the the below WiFi Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ .","title":"Pre-Requisite"},{"location":"getting-started/setting-up-pi/#connect-to-raspberry-pi","text":"Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command.","title":"Connect to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#python-setup","text":"Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/","title":"Python Setup"},{"location":"getting-started/setting-up-pi/#os-install","text":"We use HyPriot OS , a lightwieght Ubuntu-based, for our PIs, unless said otherwise. It does not have a GUI, but it will always derive the HDMI output (even if you did not connect it during boot). For making an SD card, you can use their flash tool. Below is an example: flash -u cloud-config.yml https://github.com/hypriot/image-builder-rpi/releases/download/v1.11.0/hypriotos-rpi-v1.11.0.img.zip and the content for cloud-config.yml to automatically configure user, password, and WiFi connection: #cloud-config # vim: syntax=yaml # # The current version of cloud-init in the Hypriot rpi-64 is 0.7.9 # When dealing with cloud-init, it is SUPER important to know the version # I have wasted many hours creating servers to find out the module I was trying to use wasn't in the cloud-init version I had # Documentation: http://cloudinit.readthedocs.io/en/0.7.9/index.html # Set your hostname here, the manage_etc_hosts will update the hosts file entries as well hostname: pi manage_etc_hosts: true # You could modify this for your own user information users: - name: pi gecos: \"Hypriot Pirate\" sudo: ALL=(ALL) NOPASSWD:ALL shell: /bin/bash groups: users,docker,video plain_text_passwd: raspberry lock_passwd: false ssh_pwauth: true chpasswd: { expire: false } # # Set the locale of the system # locale: \"en_US.UTF-8\" # # Set the timezone # # Value of 'timezone' must exist in /usr/share/zoneinfo # timezone: \"America/Los_Angeles\" # # Update apt packages on first boot # package_update: true # package_upgrade: true # package_reboot_if_required: true package_upgrade: false # # Install any additional apt packages you need here # packages: # - ntp # # WiFi connect to HotSpot # To make wifi work with RPi3 and RPi0 # you also have to set \"enable_uart=0\" in config.txt # See no-uart-config.txt for an example. # # # - use `wpa_passphrase SSID PASSWORD` to encrypt the psk write_files: - content: | allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf iface default inet dhcp path: /etc/network/interfaces.d/wlan0 - content: | country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"NETGEAR79\" psk=\"78zBJr!4bVdpaFIQ\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN } path: /etc/wpa_supplicant/wpa_supplicant.conf # These commands will be ran once on first boot only runcmd: # Pickup the hostname changes - 'systemctl restart avahi-daemon' # Activate WiFi interface - 'ifup wlan0'","title":"OS Install"},{"location":"getting-started/setting-up-pi/#connect-to-wifi-with-terminal","text":"edit the file /etc/wpa_supplicant/wpa_supplicant.conf with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf . Copy below info: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=US network={ ssid=\"NETGEAR79\" psk=\"78zBJr!4bVdpaFIQ\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN } Exit with ctrl^x, Enter.","title":"Connect to Wifi with Terminal"},{"location":"getting-started/speaker-mic/","text":"Speaker and Mic Setup Author: Ramyad Date: 7/25/2019 List All devices: cat /proc/asound/cards List all Playback Devices aplay -l See your card and device number Note: you need to set volume alsamixer -c <card_number> Note: you can also use pacmd set-source-volume <index> <volume> List all Recording Devices arecord -l or pacmd list-sources See your card and device number Control devices with alsamixer , use F6 to select your device:wq Set your Recording and Playback Device as the Default PCM Devices, in /etc/asound.conf pcm.!default { type asym capture.pcm \"mic\" playback.pcm \"speaker\" } pcm.mic { type plug slave { pcm \"hw:<card number>,<device number>\" } } pcm.speaker { type plug slave { pcm \"hw:<card number>,<device number>\" } } Live Streaming arecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000 Testing Playback speaker-test -t wav -c 2 Test Recording Device arecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw aplay --format=S16_LE --rate=16000 out.raw Speaker and Mic Volume Control alsamixer -c \"card number\"","title":"Setting Up Speaker and Mic"},{"location":"getting-started/speaker-mic/#speaker-and-mic-setup","text":"Author: Ramyad Date: 7/25/2019 List All devices: cat /proc/asound/cards List all Playback Devices aplay -l See your card and device number Note: you need to set volume alsamixer -c <card_number> Note: you can also use pacmd set-source-volume <index> <volume> List all Recording Devices arecord -l or pacmd list-sources See your card and device number Control devices with alsamixer , use F6 to select your device:wq Set your Recording and Playback Device as the Default PCM Devices, in /etc/asound.conf pcm.!default { type asym capture.pcm \"mic\" playback.pcm \"speaker\" } pcm.mic { type plug slave { pcm \"hw:<card number>,<device number>\" } } pcm.speaker { type plug slave { pcm \"hw:<card number>,<device number>\" } } Live Streaming arecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000 Testing Playback speaker-test -t wav -c 2 Test Recording Device arecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw aplay --format=S16_LE --rate=16000 out.raw Speaker and Mic Volume Control alsamixer -c \"card number\"","title":"Speaker and Mic Setup"},{"location":"imu/gps/","text":"NEO 6M GPS Setup Raspberry PI uses the UART interface as a serial console by default. We have to deactivate this first! Connect to the PI and run: sudo raspi-config Option 5, P6 (Serial), No, Yes sudo reboot Then, we need to connect the GPS module to the Raspberry PI. There are only 4 wires (F to F), so it's a simple connection. Use https://pinout.xyz/ if you don't know the numbering. Neo-6M RPI: - VCC to Pin 1, which is 3.3v - TX to Pin 10, which is RX (GPIO15) - RX to Pin 8, Which is TX (GPIO14) - Gnd to Pin 6, which is Gnd sudo killall gpsd sudo gpsd /dev/serial0 -F /var/run/gpsd.sock sudo cat /dev/serial0 You should be able to see some messages: $GPRMC,,V,,,,,,,,,,N*53 $GPVTG,,,,,,,,,N*30 $GPGGA,,,,,,0,00,99.99,,,,,,*48 $GPGSA,A,1,,,,,,,,,,,,,99.99,99.99,99.99*30 $GPGSV,1,1,01,02,,,20*78 For Terminal Output Then, run: sudo gpsmon /dev/serial0 or sudo cgps -s References: - 1 - 2 - 3 - 4","title":"GPS (Neo 6M GPS)"},{"location":"imu/gps/#neo-6m-gps","text":"","title":"NEO 6M GPS"},{"location":"imu/gps/#setup","text":"Raspberry PI uses the UART interface as a serial console by default. We have to deactivate this first! Connect to the PI and run: sudo raspi-config Option 5, P6 (Serial), No, Yes sudo reboot Then, we need to connect the GPS module to the Raspberry PI. There are only 4 wires (F to F), so it's a simple connection. Use https://pinout.xyz/ if you don't know the numbering. Neo-6M RPI: - VCC to Pin 1, which is 3.3v - TX to Pin 10, which is RX (GPIO15) - RX to Pin 8, Which is TX (GPIO14) - Gnd to Pin 6, which is Gnd sudo killall gpsd sudo gpsd /dev/serial0 -F /var/run/gpsd.sock sudo cat /dev/serial0 You should be able to see some messages: $GPRMC,,V,,,,,,,,,,N*53 $GPVTG,,,,,,,,,N*30 $GPGGA,,,,,,0,00,99.99,,,,,,*48 $GPGSA,A,1,,,,,,,,,,,,,99.99,99.99,99.99*30 $GPGSV,1,1,01,02,,,20*78","title":"Setup"},{"location":"imu/gps/#for-terminal-output","text":"Then, run: sudo gpsmon /dev/serial0 or sudo cgps -s References: - 1 - 2 - 3 - 4","title":"For Terminal Output"},{"location":"imu/imu/","text":"You can find every guide and how to connect to RPi here Here is the first tutorial on how to use Gyro and Accelerometer.","title":"IMU (Berry IMU)"},{"location":"irobot/keyboard/","text":"Control with Keyboard Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Keyboard Control"},{"location":"irobot/keyboard/#control-with-keyboard","text":"Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Control with Keyboard"},{"location":"irobot/navi_lidar_voice/","text":"","title":"Navigation with Lidar/Voice"},{"location":"mapping/lidar-slam/","text":"Requirements BreezySLAM PyRoboViz Python 3 Operating System -Ubuntu (18.04) Does not work with Windows Installation Clone or Download the above git repository cd to BreezySLAM/python and run the following command $ sudo python3 setup.py install For installing the rplidar python package run the following pip command $ sudo pip3 install rplidar For seeing the live animation of the map, we install PyRoboViz cd to PyRoboViz folder and run the following command sudo python3 setup.py install Running Slam Connect the lidar to the system via a USB port and run the following command in the terminal sudo chmod 666 /dev/ttyUSB0 From the BreezySLAM/examples folder run the rpslam .py file to get live map of the surrounding.","title":"Lidar"},{"location":"mapping/lidar-slam/#requirements","text":"BreezySLAM PyRoboViz Python 3 Operating System -Ubuntu (18.04) Does not work with Windows","title":"Requirements"},{"location":"mapping/lidar-slam/#installation","text":"Clone or Download the above git repository cd to BreezySLAM/python and run the following command $ sudo python3 setup.py install For installing the rplidar python package run the following pip command $ sudo pip3 install rplidar For seeing the live animation of the map, we install PyRoboViz cd to PyRoboViz folder and run the following command sudo python3 setup.py install","title":"Installation"},{"location":"mapping/lidar-slam/#running-slam","text":"Connect the lidar to the system via a USB port and run the following command in the terminal sudo chmod 666 /dev/ttyUSB0 From the BreezySLAM/examples folder run the rpslam .py file to get live map of the surrounding.","title":"Running Slam"},{"location":"people/people/","text":"Our Group Faculty * Hyesoon Kim Graduate Students Ramyad Hadidi Jiashen Cao Undergraduate Students Fall 2017 Jiashen Cao Matthew Woodward Spring 2018 Jiashen Cao Fall 2018 Chunjun Jia Spring 2019 Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia Summer 2019 Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"People"},{"location":"people/people/#our-group","text":"","title":"Our Group"},{"location":"people/people/#faculty","text":"","title":"Faculty"},{"location":"people/people/#hyesoon-kim","text":"","title":"* Hyesoon Kim"},{"location":"people/people/#graduate-students","text":"Ramyad Hadidi Jiashen Cao","title":"Graduate Students"},{"location":"people/people/#undergraduate-students","text":"","title":"Undergraduate Students"},{"location":"people/people/#fall-2017","text":"Jiashen Cao Matthew Woodward","title":"Fall 2017"},{"location":"people/people/#spring-2018","text":"Jiashen Cao","title":"Spring 2018"},{"location":"people/people/#fall-2018","text":"Chunjun Jia","title":"Fall 2018"},{"location":"people/people/#spring-2019","text":"Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia","title":"Spring 2019"},{"location":"people/people/#summer-2019","text":"Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"Summer 2019"},{"location":"speech/deepspeech/","text":"Deepspeech on Raspberry Pi Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package Run Deepspeech with Trained Model (use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download Making Your Own Model Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Deepspeech"},{"location":"speech/deepspeech/#deepspeech-on-raspberry-pi","text":"Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package","title":"Deepspeech on Raspberry Pi"},{"location":"speech/deepspeech/#run-deepspeech-with-trained-model","text":"(use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download","title":"Run Deepspeech with Trained Model"},{"location":"speech/deepspeech/#making-your-own-model","text":"Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Making Your Own Model"},{"location":"speech/klauba-speech/","text":"Klauba Speech Author: Bryce Plunkett Introduction In this article, I will go over how to get Klauba's (current) iteration of speech recongition code to run on the Raspberry PI or an Ubuntu-based machine. BEFORE you read this article, I encourage you to check out the Sphinx doc. All of the code/scripts are iterations and improvements of that code. Major Changes Implemented a hotword detector (Listens for \"Klauba\") before recording and parsing speech input \"Intelligent\" voice recording after \"Klauba\" is detected Wont' start recording until silence has been broken Stops recording after a certain amount of continous silence Improved accuracy of speech recognition Fine-tuned the corpus (and regenerated language and lexical models) Created a script to autogenerate corpuses based off of sententence templates and room numbers Removed the pre-input noise filtering code (in the previous iteration, we noise filtered the code BEFORE we passed it to shpinx) Parsed speech now passed to a python script, which can distribute the text to all other code (like the movement code) Script won't stop running until stopped by the user Installing Dependencies and Code Base NOTE: This can be a little tricky. Every system is different and requires a slightly different install process. If you run into any problems, feel free to shoot me an e-mail: plunkett.bryce.m@gatech.edu . Snowboy refers to the library used for hotword detection Sphinx (Pocketsphinx, CMUSphinx) refers to the library used for speech recognition Supported Operating Systems and Prerequisites: This OS list has been pulled from Snowboy git docs. The operating systems marked with a \"*\", have tested the code on all versions of Raspberry Pi (with Raspbian based on Debian Jessie 8.0) 64bit Ubuntu 14.04* 64bit Mac OS X ARM64 (aarch64, Ubuntu 16.04)* You also need to have Python 2.7 and git installed. Ultimately, this guide assumes you are on an Ubuntu-based operating system Configuring your microphone You might need to redo this step whenever a USB device with a microphone (including webcams) is plugged into the machine you're running the code on. To Test if your microphone is configured correctly: rec {{file name}}.wav Hit contrl-c when you're done recording. If you hear your voice in the generated .wav , then your microphone is configured correctly. If not, then your microphone is not configured correctly. If your microphone is not configured correctly: Edit (or create if it does not exist): /etc/asound.conf Copy and paste into the file and edit the playback card and record card. You will need to do this as the root user . To get playback devices: aplay -l To get recording devices: arecord -l pcm.!default { type asym playback.pcm { type plug slave.pcm \"hw {{playback card #}}, {{playback device #}}\" } capture.pcm { type plug slave.pcm \"hw {{record card #}}, {{record device #}}\" } } Example /etc/asound.conf : pcm.!default { type asym playback.pcm { type plug slave.pcm \"hw:0,0\" } capture.pcm { type plug slave.pcm \"hw:1,0\" } } Test your changes with the arec command Installing Snowboy First, you need to install Snowboy (the library used for hotword detection). This tends to always pose some complications. You can try following the guide below or the official Snowboy guide . I recommend trying to follow the Snowboy guide and using this guide as a reference. Installing Swig You need a version of swig installed >= 3.0.10 . I have tried older versions with no success. Snowboy will refuse to completely build. Unfortunately, the newer version of swig does not seem to be on Ubuntu's apt repository, so you will need to manually build and install it. Here are several resources to help you install swig: * http://www.linuxfromscratch.org/blfs/view/7.10/general/swig.html * https://github.com/swig/swig/blob/master/INSTALL * https://github.com/Kitt-AI/snowboy (scroll to the section that details installing swig) Verify your swig install : swig -version Install other snowboy dependencies $ sudo apt-get install python-pyaudio sox $ pip install pyaudio $ sudo apt-get install libatlas-base-dev Compiling Python wrapper Clone the Snowboy repository git clone https://github.com/Kitt-AI/snowboy.git cd snowboy Compile the Python wrapper (assuming you're cd'ed into the snowboy root directory) cd swig/Python make Setting Environment Variable Ideally, you successfully compiled the Snowboy python wrapper. Now, our python needs a way to access what you compiled. You need to add {{Snowboy Root Directory}}/examples/Python to the PYTHONPATH environment variable. Whenever you run a command in python, one place it will search for depencies are all the directories listed in the PYTHONPATH . Replace {{Snowboy Root Directory}} with an absolute path to your Snowboy code. To test you set this environment variable correctly, open up a new terminal and type: echo $PYTHONPATH . Your path to the Python Snowboy wrapper should now be in list of variables in the PYTHONPATH. Installing Our Code Clone and install dependencies: $ git clone https://github.com/parallel-ml/sphinxSpeech2Text $ cd ./sphinxSpeech2Text $ sudo chmod u+x ./install.sh $ ./install.sh Compile $ make Before you can run the code: Set the SPEECH_RECOGNITION environment variable to point to the repository. For instance, if you cloned the repository into ~ , then the SPEECH_RECOGNITION environment variable should be set to ~/sphinxSpeech2Text . DO NOT USE any shorthand such as ~ in the environment variable Running Code For the First Time To run the code, call ./snowboy-code . All processed output will be saved to Fixing errors You will probably see many errors related to audio dependencies on your machine. This issue thread on the Snowboy Github repository does a great job solving most of them. Please refer to it FIRST before continuing with this guide Jack Control X11 Issue If jack_control start throws some issue about X11 adapter, it's because you're SSHing into (presumably a raspberry pi) without X11 forwarding (don't ask me why jack_control wants a display). To fix this issue, when you SSH into the machine you're installing the code on, use the \"-X\" (case sensitive) parameter: ssh -X {{User name}}@{{IP}} . Cannot lock down byte memory The error regarding \"byte memory\" appears to not affect how the code runs. It can be ignored (it's a glitch with snowboy) The console hangs/freezes when \"Klauba\" is detected If the console hangs after detecting Klauba, in other words, nothing happens after INFO:snowboy:Keyword 1 detected at time: {{DATE} {{TIME}} : This is a bizarre glitch with Snowboy. Luckily, there is a fix... 1. Hit control-z (exits the script) 2. Edit klauba.py and comment line #12 out ( detector.terminate() ) ( On the Raspberry pis, this line NEEDS to run. It should not be commented out on the Raspberry PIs ) 3. Restart your machine Code Explanation All paths in the following explanation will treat the repository as the root directory /snowboy-decode.c : This is the main script. It calls all python \"helper\" scripts and also make the call to the pocketshpinx library to decode the data. It runs infinitely until the user kills the script with a process signal. You can change the lexical model (pronunciation) dictionary and language model passed to Klauba in this code at lines #21 and #22, respectively. /klauba.py : This is the hotword detection script. It uses the Snowboy python wrapper. If it detects the \"Klauba\" hotword, it calls the /record.py script /record.py : This script won't start recording until a break in silence is detected. Once it starts recording, it won't stop recording until a certain amount of configured silence. This idea of continous silence follows the \"leaky bucket\" design pattern where the water is silence. Silence fills the bucket, while noise empties the bucket. If the bucket overflows, then the recording stops and it will be saved to /inputs/raw_recording.wav . Once this script ends, snowboy-decode.c will call the pocketsphinx library to decode the recording. /output/decode-details.log : A log file generated by pocketsphinx decodes the audio into text. If there any errors with your pocketsphinx install or the parameters (such as the models) you pass in, they will be logged here. /output/log.txt : A timestamped log of the words pocketsphinx decodes. It is generated by the /process-output.py script /process-output.py : The output of pocketsphinx is passed to this script via a pipe in a bash command (standard input). The process_speech_input function receives this input as a string. Modify this function as necessary to handle what you want to do with the user's parsed speech input Models /models/Klauba.pmdl : This is the model that Snowboy uses for detecting the Klauba hotword. The current model can be downloaded/retrained at this link /models/corpus/corpus-generator.py : This script will autogenerate a corpus ( /models/corpus/corpus.txt ) for you based on: * Template strings (encoded within the script itself) that take in a room numbers * A list of general phrases /models/corpus/general-phrases.txt (no room numbers in these phrases) * A list of room numbers (that are inserted into the templates) /models/corpus/rooms.txt models/corpus/Corpus_V3 : This directory contains the dictionary and language models currently being used by sphinx in the script. Updating the Dictionary and Language Models Once you have a corpus you want to use ( see /models/corpus/corpus-generator.py ): 1. Use this website to create a dictionary and language model based on an inputted corpus 2. Download the {{name}}.lm and {{name}}.dic and save them to a directory. Recommended directory to save the new models to: /models/corpus/Corpus_V{{corpus version number}}. **Be sure to include the corpus.txt in this directory to keep track of what the models were trained with!** 3. Edit /snowboy-decode.c and update line #21 with the new language model path and line #22 with the new dictionary model path 4. Don't forget to rebuid the c script. Call make`. Future Improvements Currently, the /record.py script cuts off the beginnning of user input (it's not recorded). As a result, sometimes the sphinx doesn't parse the first words of what the user says. For instance, the user will say, \"Take me to room 2443\" and sphinx wil parse it as \"2443\" or \"ROOM 2443\".","title":"Klauba Speech"},{"location":"speech/klauba-speech/#klauba-speech","text":"Author: Bryce Plunkett","title":"Klauba Speech"},{"location":"speech/klauba-speech/#introduction","text":"In this article, I will go over how to get Klauba's (current) iteration of speech recongition code to run on the Raspberry PI or an Ubuntu-based machine. BEFORE you read this article, I encourage you to check out the Sphinx doc. All of the code/scripts are iterations and improvements of that code.","title":"Introduction"},{"location":"speech/klauba-speech/#major-changes","text":"Implemented a hotword detector (Listens for \"Klauba\") before recording and parsing speech input \"Intelligent\" voice recording after \"Klauba\" is detected Wont' start recording until silence has been broken Stops recording after a certain amount of continous silence Improved accuracy of speech recognition Fine-tuned the corpus (and regenerated language and lexical models) Created a script to autogenerate corpuses based off of sententence templates and room numbers Removed the pre-input noise filtering code (in the previous iteration, we noise filtered the code BEFORE we passed it to shpinx) Parsed speech now passed to a python script, which can distribute the text to all other code (like the movement code) Script won't stop running until stopped by the user","title":"Major Changes"},{"location":"speech/klauba-speech/#installing-dependencies-and-code-base","text":"NOTE: This can be a little tricky. Every system is different and requires a slightly different install process. If you run into any problems, feel free to shoot me an e-mail: plunkett.bryce.m@gatech.edu . Snowboy refers to the library used for hotword detection Sphinx (Pocketsphinx, CMUSphinx) refers to the library used for speech recognition","title":"Installing Dependencies and Code Base"},{"location":"speech/klauba-speech/#supported-operating-systems-and-prerequisites","text":"This OS list has been pulled from Snowboy git docs. The operating systems marked with a \"*\", have tested the code on all versions of Raspberry Pi (with Raspbian based on Debian Jessie 8.0) 64bit Ubuntu 14.04* 64bit Mac OS X ARM64 (aarch64, Ubuntu 16.04)* You also need to have Python 2.7 and git installed. Ultimately, this guide assumes you are on an Ubuntu-based operating system","title":"Supported Operating Systems and Prerequisites:"},{"location":"speech/klauba-speech/#configuring-your-microphone","text":"You might need to redo this step whenever a USB device with a microphone (including webcams) is plugged into the machine you're running the code on. To Test if your microphone is configured correctly: rec {{file name}}.wav Hit contrl-c when you're done recording. If you hear your voice in the generated .wav , then your microphone is configured correctly. If not, then your microphone is not configured correctly. If your microphone is not configured correctly: Edit (or create if it does not exist): /etc/asound.conf Copy and paste into the file and edit the playback card and record card. You will need to do this as the root user . To get playback devices: aplay -l To get recording devices: arecord -l pcm.!default { type asym playback.pcm { type plug slave.pcm \"hw {{playback card #}}, {{playback device #}}\" } capture.pcm { type plug slave.pcm \"hw {{record card #}}, {{record device #}}\" } } Example /etc/asound.conf : pcm.!default { type asym playback.pcm { type plug slave.pcm \"hw:0,0\" } capture.pcm { type plug slave.pcm \"hw:1,0\" } } Test your changes with the arec command","title":"Configuring your microphone"},{"location":"speech/klauba-speech/#installing-snowboy","text":"First, you need to install Snowboy (the library used for hotword detection). This tends to always pose some complications. You can try following the guide below or the official Snowboy guide . I recommend trying to follow the Snowboy guide and using this guide as a reference.","title":"Installing Snowboy"},{"location":"speech/klauba-speech/#installing-swig","text":"You need a version of swig installed >= 3.0.10 . I have tried older versions with no success. Snowboy will refuse to completely build. Unfortunately, the newer version of swig does not seem to be on Ubuntu's apt repository, so you will need to manually build and install it. Here are several resources to help you install swig: * http://www.linuxfromscratch.org/blfs/view/7.10/general/swig.html * https://github.com/swig/swig/blob/master/INSTALL * https://github.com/Kitt-AI/snowboy (scroll to the section that details installing swig) Verify your swig install : swig -version","title":"Installing Swig"},{"location":"speech/klauba-speech/#install-other-snowboy-dependencies","text":"$ sudo apt-get install python-pyaudio sox $ pip install pyaudio $ sudo apt-get install libatlas-base-dev","title":"Install other snowboy dependencies"},{"location":"speech/klauba-speech/#compiling-python-wrapper","text":"Clone the Snowboy repository git clone https://github.com/Kitt-AI/snowboy.git cd snowboy Compile the Python wrapper (assuming you're cd'ed into the snowboy root directory) cd swig/Python make","title":"Compiling Python wrapper"},{"location":"speech/klauba-speech/#setting-environment-variable","text":"Ideally, you successfully compiled the Snowboy python wrapper. Now, our python needs a way to access what you compiled. You need to add {{Snowboy Root Directory}}/examples/Python to the PYTHONPATH environment variable. Whenever you run a command in python, one place it will search for depencies are all the directories listed in the PYTHONPATH . Replace {{Snowboy Root Directory}} with an absolute path to your Snowboy code. To test you set this environment variable correctly, open up a new terminal and type: echo $PYTHONPATH . Your path to the Python Snowboy wrapper should now be in list of variables in the PYTHONPATH.","title":"Setting Environment Variable"},{"location":"speech/klauba-speech/#installing-our-code","text":"","title":"Installing Our Code"},{"location":"speech/klauba-speech/#clone-and-install-dependencies","text":"$ git clone https://github.com/parallel-ml/sphinxSpeech2Text $ cd ./sphinxSpeech2Text $ sudo chmod u+x ./install.sh $ ./install.sh","title":"Clone and install dependencies:"},{"location":"speech/klauba-speech/#compile","text":"$ make","title":"Compile"},{"location":"speech/klauba-speech/#before-you-can-run-the-code","text":"Set the SPEECH_RECOGNITION environment variable to point to the repository. For instance, if you cloned the repository into ~ , then the SPEECH_RECOGNITION environment variable should be set to ~/sphinxSpeech2Text . DO NOT USE any shorthand such as ~ in the environment variable","title":"Before you can run the code:"},{"location":"speech/klauba-speech/#running-code-for-the-first-time","text":"To run the code, call ./snowboy-code . All processed output will be saved to","title":"Running Code For the First Time"},{"location":"speech/klauba-speech/#fixing-errors","text":"You will probably see many errors related to audio dependencies on your machine. This issue thread on the Snowboy Github repository does a great job solving most of them. Please refer to it FIRST before continuing with this guide","title":"Fixing errors"},{"location":"speech/klauba-speech/#jack-control-x11-issue","text":"If jack_control start throws some issue about X11 adapter, it's because you're SSHing into (presumably a raspberry pi) without X11 forwarding (don't ask me why jack_control wants a display). To fix this issue, when you SSH into the machine you're installing the code on, use the \"-X\" (case sensitive) parameter: ssh -X {{User name}}@{{IP}} .","title":"Jack Control X11 Issue"},{"location":"speech/klauba-speech/#cannot-lock-down-byte-memory","text":"The error regarding \"byte memory\" appears to not affect how the code runs. It can be ignored (it's a glitch with snowboy)","title":"Cannot lock down byte memory"},{"location":"speech/klauba-speech/#the-console-hangsfreezes-when-klauba-is-detected","text":"If the console hangs after detecting Klauba, in other words, nothing happens after INFO:snowboy:Keyword 1 detected at time: {{DATE} {{TIME}} : This is a bizarre glitch with Snowboy. Luckily, there is a fix... 1. Hit control-z (exits the script) 2. Edit klauba.py and comment line #12 out ( detector.terminate() ) ( On the Raspberry pis, this line NEEDS to run. It should not be commented out on the Raspberry PIs ) 3. Restart your machine","title":"The console hangs/freezes when \"Klauba\" is detected"},{"location":"speech/klauba-speech/#code-explanation","text":"All paths in the following explanation will treat the repository as the root directory /snowboy-decode.c : This is the main script. It calls all python \"helper\" scripts and also make the call to the pocketshpinx library to decode the data. It runs infinitely until the user kills the script with a process signal. You can change the lexical model (pronunciation) dictionary and language model passed to Klauba in this code at lines #21 and #22, respectively. /klauba.py : This is the hotword detection script. It uses the Snowboy python wrapper. If it detects the \"Klauba\" hotword, it calls the /record.py script /record.py : This script won't start recording until a break in silence is detected. Once it starts recording, it won't stop recording until a certain amount of configured silence. This idea of continous silence follows the \"leaky bucket\" design pattern where the water is silence. Silence fills the bucket, while noise empties the bucket. If the bucket overflows, then the recording stops and it will be saved to /inputs/raw_recording.wav . Once this script ends, snowboy-decode.c will call the pocketsphinx library to decode the recording. /output/decode-details.log : A log file generated by pocketsphinx decodes the audio into text. If there any errors with your pocketsphinx install or the parameters (such as the models) you pass in, they will be logged here. /output/log.txt : A timestamped log of the words pocketsphinx decodes. It is generated by the /process-output.py script /process-output.py : The output of pocketsphinx is passed to this script via a pipe in a bash command (standard input). The process_speech_input function receives this input as a string. Modify this function as necessary to handle what you want to do with the user's parsed speech input","title":"Code Explanation"},{"location":"speech/klauba-speech/#models","text":"/models/Klauba.pmdl : This is the model that Snowboy uses for detecting the Klauba hotword. The current model can be downloaded/retrained at this link /models/corpus/corpus-generator.py : This script will autogenerate a corpus ( /models/corpus/corpus.txt ) for you based on: * Template strings (encoded within the script itself) that take in a room numbers * A list of general phrases /models/corpus/general-phrases.txt (no room numbers in these phrases) * A list of room numbers (that are inserted into the templates) /models/corpus/rooms.txt models/corpus/Corpus_V3 : This directory contains the dictionary and language models currently being used by sphinx in the script.","title":"Models"},{"location":"speech/klauba-speech/#updating-the-dictionary-and-language-models","text":"Once you have a corpus you want to use ( see /models/corpus/corpus-generator.py ): 1. Use this website to create a dictionary and language model based on an inputted corpus 2. Download the {{name}}.lm and {{name}}.dic and save them to a directory. Recommended directory to save the new models to: /models/corpus/Corpus_V{{corpus version number}}. **Be sure to include the corpus.txt in this directory to keep track of what the models were trained with!** 3. Edit /snowboy-decode.c and update line #21 with the new language model path and line #22 with the new dictionary model path 4. Don't forget to rebuid the c script. Call make`.","title":"Updating the Dictionary and Language Models"},{"location":"speech/klauba-speech/#future-improvements","text":"Currently, the /record.py script cuts off the beginnning of user input (it's not recorded). As a result, sometimes the sphinx doesn't parse the first words of what the user says. For instance, the user will say, \"Take me to room 2443\" and sphinx wil parse it as \"2443\" or \"ROOM 2443\".","title":"Future Improvements"},{"location":"speech/sphinx/","text":"CMU Sphinx Authors: Ramyad, Sayuj (Updated by Bryce Plunkett; Fall 2019) Date: 7/25/2019 Set-Up (Fast) Prerequisites: - Python 2.7 - Numpy - Ubuntu-Based OS (Ubuntu, PopOS...) Clone and install dependencies: $ git clone https://github.com/parallel-ml/sphinxSpeech2Text $ cd ./sphinxSpeech2Text $ sudo chmod u+x ./install.sh $ ./install.sh Before compiling the C code: - You might need to edit line 12 and the arecord command parameters to match your environment (such as changing the recording device) Compile : $ make Before you can run the code: - Set the SPEECH_RECOGNITION environment variable to point to the repository. For instance, if you cloned the repository into ~ , then the SPEECH_RECOGNITION environment variable should be set to ~/sphinxSpeech2Text Running the code: You now have compiled the code. A demo can be run via ./decode . It will record your voice for 5 seconds, filter the noise, and try to parse commands from the limited corpus. The output text can be found in the /output folder and the raw and filtered recordings can be found in the /inputs folder. Details Parallel-ml repo: https://github.com/parallel-ml/sphinxSpeech2Text I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: link Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic , which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary an language model using a tool I found online link , specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file. Example of Running in Terminal pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm Note: If you get an error such as: error while loading shared libraries: libpocketsphinx.so.3 , you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below: export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig Installation sudo apt-get install bison sudo apt-get install swig cd sphinxbase-5prealpha ./autogen.sh .configure make sudo make install export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig cd ../pocketsphinx-5prealpha ./autogen.sh .configure make sudo make install Example of Running with C Contents of decode.c gcc -o decode decode.c #include <stdlib.h> #include <stdio.h> #include <time.h> #define BILLION 1000000000.0; int main(void) { struct timespec start, end; system(\"export LD_LIBRARY_PATH=/usr/local/lib\"); system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\"); system(\"echo done recording...\"); system(\"python testfiles/noiseClean.py\"); system(\"echo done cleaning...\"); clock_gettime(CLOCK_REALTIME, &start); system(\"\\ pocketsphinx_continuous \\ -infile testfiles/filtered.wav \\ -dict dicts/8050.dic \\ -lm dicts/8050.lm \\ 2>./output/unwanted-stuff.log | tee ./output/words.txt\"); // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt system(\"echo done decoding...\"); clock_gettime(CLOCK_REALTIME, &end); double time_spent = (end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / BILLION; char *timerOutput = malloc(25); sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent); system(timerOutput); } System Mic Noise Fix Using system/USB mic has noises, to clean, here is the content of noiseClean.py: outname = 'testfiles/filtered.wav' cutOffFrequency = 400.0 # from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean def running_mean(x, windowSize): cumsum = np.cumsum(np.insert(x, 0, 0)) return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize # from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174 def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True): if sample_width == 1: dtype = np.uint8 # unsigned char elif sample_width == 2: dtype = np.int16 # signed 2-byte short else: raise ValueError(\"Only supports 8 and 16 bit audio formats.\") channels = np.fromstring(raw_bytes, dtype=dtype) if interleaved: # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data channels.shape = (n_frames, n_channels) channels = channels.T else: # channels are not interleaved. All samples from channel M occur before all samples from channel M-1 channels.shape = (n_channels, n_frames) return channels with contextlib.closing(wave.open(fname,'rb')) as spf: sampleRate = spf.getframerate() ampWidth = spf.getsampwidth() nChannels = spf.getnchannels() nFrames = spf.getnframes() # Extract Raw Audio from multi-channel Wav File signal = spf.readframes(nFrames*nChannels) spf.close() channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True) # get window size # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter freqRatio = (cutOffFrequency/sampleRate) N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio) # Use moviung average (only on first channel) filtered = running_mean(channels[0], N).astype(channels.dtype) wav_file = wave.open(outname, \"w\") wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname())) wav_file.writeframes(filtered.tobytes('C')) wav_file.close()","title":"Sphinx"},{"location":"speech/sphinx/#cmu-sphinx","text":"Authors: Ramyad, Sayuj (Updated by Bryce Plunkett; Fall 2019) Date: 7/25/2019","title":"CMU Sphinx"},{"location":"speech/sphinx/#set-up-fast","text":"Prerequisites: - Python 2.7 - Numpy - Ubuntu-Based OS (Ubuntu, PopOS...) Clone and install dependencies: $ git clone https://github.com/parallel-ml/sphinxSpeech2Text $ cd ./sphinxSpeech2Text $ sudo chmod u+x ./install.sh $ ./install.sh Before compiling the C code: - You might need to edit line 12 and the arecord command parameters to match your environment (such as changing the recording device) Compile : $ make Before you can run the code: - Set the SPEECH_RECOGNITION environment variable to point to the repository. For instance, if you cloned the repository into ~ , then the SPEECH_RECOGNITION environment variable should be set to ~/sphinxSpeech2Text Running the code: You now have compiled the code. A demo can be run via ./decode . It will record your voice for 5 seconds, filter the noise, and try to parse commands from the limited corpus. The output text can be found in the /output folder and the raw and filtered recordings can be found in the /inputs folder.","title":"Set-Up (Fast)"},{"location":"speech/sphinx/#details","text":"Parallel-ml repo: https://github.com/parallel-ml/sphinxSpeech2Text I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: link Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic , which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary an language model using a tool I found online link , specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file.","title":"Details"},{"location":"speech/sphinx/#example-of-running-in-terminal","text":"pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm Note: If you get an error such as: error while loading shared libraries: libpocketsphinx.so.3 , you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below: export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig","title":"Example of Running in Terminal"},{"location":"speech/sphinx/#installation","text":"sudo apt-get install bison sudo apt-get install swig cd sphinxbase-5prealpha ./autogen.sh .configure make sudo make install export LD_LIBRARY_PATH=/usr/local/lib export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig cd ../pocketsphinx-5prealpha ./autogen.sh .configure make sudo make install","title":"Installation"},{"location":"speech/sphinx/#example-of-running-with-c","text":"Contents of decode.c gcc -o decode decode.c #include <stdlib.h> #include <stdio.h> #include <time.h> #define BILLION 1000000000.0; int main(void) { struct timespec start, end; system(\"export LD_LIBRARY_PATH=/usr/local/lib\"); system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\"); system(\"echo done recording...\"); system(\"python testfiles/noiseClean.py\"); system(\"echo done cleaning...\"); clock_gettime(CLOCK_REALTIME, &start); system(\"\\ pocketsphinx_continuous \\ -infile testfiles/filtered.wav \\ -dict dicts/8050.dic \\ -lm dicts/8050.lm \\ 2>./output/unwanted-stuff.log | tee ./output/words.txt\"); // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt system(\"echo done decoding...\"); clock_gettime(CLOCK_REALTIME, &end); double time_spent = (end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / BILLION; char *timerOutput = malloc(25); sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent); system(timerOutput); }","title":"Example of Running with C"},{"location":"speech/sphinx/#system-mic-noise-fix","text":"Using system/USB mic has noises, to clean, here is the content of noiseClean.py: outname = 'testfiles/filtered.wav' cutOffFrequency = 400.0 # from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean def running_mean(x, windowSize): cumsum = np.cumsum(np.insert(x, 0, 0)) return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize # from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174 def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True): if sample_width == 1: dtype = np.uint8 # unsigned char elif sample_width == 2: dtype = np.int16 # signed 2-byte short else: raise ValueError(\"Only supports 8 and 16 bit audio formats.\") channels = np.fromstring(raw_bytes, dtype=dtype) if interleaved: # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data channels.shape = (n_frames, n_channels) channels = channels.T else: # channels are not interleaved. All samples from channel M occur before all samples from channel M-1 channels.shape = (n_channels, n_frames) return channels with contextlib.closing(wave.open(fname,'rb')) as spf: sampleRate = spf.getframerate() ampWidth = spf.getsampwidth() nChannels = spf.getnchannels() nFrames = spf.getnframes() # Extract Raw Audio from multi-channel Wav File signal = spf.readframes(nFrames*nChannels) spf.close() channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True) # get window size # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter freqRatio = (cutOffFrequency/sampleRate) N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio) # Use moviung average (only on first channel) filtered = running_mean(channels[0], N).astype(channels.dtype) wav_file = wave.open(outname, \"w\") wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname())) wav_file.writeframes(filtered.tobytes('C')) wav_file.close()","title":"System Mic Noise Fix"},{"location":"speech/text-to-speech/","text":"Template - Text to Speech","title":"Text to Speech"},{"location":"speech/text-to-speech/#template-text-to-speech","text":"","title":"Template - Text to Speech"},{"location":"vision/character/","text":"Character Recognition Authors: Akanksha Date: 8/01/2019 1) Introduction related to the domain area of the project Our project is an extended work on the blooming Computer Vision which was related to the implementation of a version of autonomous robot using Roomba with Raspberry PIs. We implement a system capable of accounting for different characters present in an image, specially room numbers. We have used the MNIST dataset for this purpose. 2) Literature Survey/background search/study of similar projects We have gone through similar projects in each module (i.e text detection, word segmentation and character prediction).After going through various links, implementing one, getting stuck in between and figuring out other ways as to how to get rid of the errors, we finally went with East text detector for the detection of text from an image which was further followed by segmentation and prediction. 3) Project Code can be found here: https://github.com/parallel-ml/wordSegmentation Diagram: Input Image -> Text Detection -> Word Segmentation -> Characters with Bounding Boxes -> Predicted Output 4) References https://github.com/EN10/KerasMNIST https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/","title":"Character Recognition"},{"location":"vision/character/#character-recognition","text":"Authors: Akanksha Date: 8/01/2019 1) Introduction related to the domain area of the project Our project is an extended work on the blooming Computer Vision which was related to the implementation of a version of autonomous robot using Roomba with Raspberry PIs. We implement a system capable of accounting for different characters present in an image, specially room numbers. We have used the MNIST dataset for this purpose. 2) Literature Survey/background search/study of similar projects We have gone through similar projects in each module (i.e text detection, word segmentation and character prediction).After going through various links, implementing one, getting stuck in between and figuring out other ways as to how to get rid of the errors, we finally went with East text detector for the detection of text from an image which was further followed by segmentation and prediction. 3) Project Code can be found here: https://github.com/parallel-ml/wordSegmentation Diagram: Input Image -> Text Detection -> Word Segmentation -> Characters with Bounding Boxes -> Predicted Output 4) References https://github.com/EN10/KerasMNIST https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/","title":"Character Recognition"},{"location":"visual_slam/calibration/","text":"Camera Calibration The cameras that we use will have some distortion. Calibration is the process of figuring out and undoing this distortion. The process is more involved with stereo cameras. For our purposes, we have created a calibration docker image Calibration To calibrate your camera, you can use the nimashoghi/calibration Docker image. Use the ./calibration command for mono calibration and ./calibration_stereo for stereo calibration. Monocular Calibraion Below is the docker-compose.yml file for mono calibration: version: \"3.1\" services: calibration_mono: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.113:0.0 volumes: - \"./calib/:/output\" - \"./assets/config.xml:/app/assets/config.xml\" Stereo Calibraion Below is the docker-compose.yml file for stereo calibration: version: \"3.1\" services: monocular: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.114:0.0 volumes: - \"./calib_stereo/:/output\" - \"./assets/:/app/assets/\" - \"../snapshots/output/:/app/assets/stereo_input/\" command: ./calibration_stereo /app/assets/stereo_calib.xml Other resources Camera calibration With OpenCV Camera Calibration and 3D Reconstruction","title":"Camera Calibration"},{"location":"visual_slam/calibration/#camera-calibration","text":"The cameras that we use will have some distortion. Calibration is the process of figuring out and undoing this distortion. The process is more involved with stereo cameras. For our purposes, we have created a calibration docker image","title":"Camera Calibration"},{"location":"visual_slam/calibration/#calibration","text":"To calibrate your camera, you can use the nimashoghi/calibration Docker image. Use the ./calibration command for mono calibration and ./calibration_stereo for stereo calibration.","title":"Calibration"},{"location":"visual_slam/calibration/#monocular-calibraion","text":"Below is the docker-compose.yml file for mono calibration: version: \"3.1\" services: calibration_mono: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.113:0.0 volumes: - \"./calib/:/output\" - \"./assets/config.xml:/app/assets/config.xml\"","title":"Monocular Calibraion"},{"location":"visual_slam/calibration/#stereo-calibraion","text":"Below is the docker-compose.yml file for stereo calibration: version: \"3.1\" services: monocular: image: nimashoghi/calibration environment: - DISPLAY=192.168.0.114:0.0 volumes: - \"./calib_stereo/:/output\" - \"./assets/:/app/assets/\" - \"../snapshots/output/:/app/assets/stereo_input/\" command: ./calibration_stereo /app/assets/stereo_calib.xml","title":"Stereo Calibraion"},{"location":"visual_slam/calibration/#other-resources","text":"Camera calibration With OpenCV Camera Calibration and 3D Reconstruction","title":"Other resources"},{"location":"visual_slam/installing-docker-on-pi/","text":"Installing Docker on the Raspberry Pi This document goes through the installation prcess for docker and coker-compose on the Raspberry Pi. Preparing Enironment Run the following command in your terminal: sudo apt-get install -y build-essential libssl-dev libffi-dev Installing Docker Run the following command in your terminal: curl -sSL https://get.docker.com | sudo sh && sudo usermod -aG docker pi Installing docker-compose Run the following command in your terminal: sudo apt-get -y install python-setuptools && sudo easy_install pip && sudo pip install docker-compose Other resources https://howchoo.com/g/nmrlzmq1ymn/how-to-install-docker-on-your-raspberry-pi https://stevenbreuls.com/2019/01/install-docker-on-raspberry-pi/","title":"Installing Docker on the Raspberry Pi"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker-on-the-raspberry-pi","text":"This document goes through the installation prcess for docker and coker-compose on the Raspberry Pi.","title":"Installing Docker on the Raspberry Pi"},{"location":"visual_slam/installing-docker-on-pi/#preparing-enironment","text":"Run the following command in your terminal: sudo apt-get install -y build-essential libssl-dev libffi-dev","title":"Preparing Enironment"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker","text":"Run the following command in your terminal: curl -sSL https://get.docker.com | sudo sh && sudo usermod -aG docker pi","title":"Installing Docker"},{"location":"visual_slam/installing-docker-on-pi/#installing-docker-compose","text":"Run the following command in your terminal: sudo apt-get -y install python-setuptools && sudo easy_install pip && sudo pip install docker-compose","title":"Installing docker-compose"},{"location":"visual_slam/installing-docker-on-pi/#other-resources","text":"https://howchoo.com/g/nmrlzmq1ymn/how-to-install-docker-on-your-raspberry-pi https://stevenbreuls.com/2019/01/install-docker-on-raspberry-pi/","title":"Other resources"},{"location":"visual_slam/live-streaming-pi-video/","text":"Live Stream Raspberry Pi Video Across the Network We have built, containerized, and uploaded the mjpeg-streamer program into the nimashoghi/streamer image. You can use this image to stream a mjpeg stream over HTTP. Using a USB Camera Run the streamer image with the input_uvc input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i 'input_uvc.so --device /dev/video0' -o output_http.so Make sure to change /dev/video0 to reflect the Linux device file path. Settings for the input_uvc plugin are shown here. Stereo Cameras For stereo cameras, you can run two separate instances of the nimashoghi/streamer image. Example docker-compose configuration is shown below: version: \"3.1\" services: video0: image: nimashoghi/streamer privileged: true ports: - \"8080:8080\" command: -i 'input_uvc.so --device /dev/video0' -o output_http.so video1: image: nimashoghi/streamer privileged: true ports: - \"8081:8080\" command: -i 'input_uvc.so --device /dev/video1' -o output_http.so Using the Pi Camera Module Run the streamer image with the input_raspicam input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i input_raspicam.so -o output_http.so Settings for the input_raspicam plugin are shown here. View the Stream To view the stream, visit the URL http://{RASPBERRY_PI_LOCAL_IP}:8080/?action=stream in your browser. You can also open this network stream with VLC to record the stream as a .avi file.","title":"Live Streaming Raspberry Pi Video"},{"location":"visual_slam/live-streaming-pi-video/#live-stream-raspberry-pi-video-across-the-network","text":"We have built, containerized, and uploaded the mjpeg-streamer program into the nimashoghi/streamer image. You can use this image to stream a mjpeg stream over HTTP.","title":"Live Stream Raspberry Pi Video Across the Network"},{"location":"visual_slam/live-streaming-pi-video/#using-a-usb-camera","text":"Run the streamer image with the input_uvc input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i 'input_uvc.so --device /dev/video0' -o output_http.so Make sure to change /dev/video0 to reflect the Linux device file path. Settings for the input_uvc plugin are shown here.","title":"Using a USB Camera"},{"location":"visual_slam/live-streaming-pi-video/#stereo-cameras","text":"For stereo cameras, you can run two separate instances of the nimashoghi/streamer image. Example docker-compose configuration is shown below: version: \"3.1\" services: video0: image: nimashoghi/streamer privileged: true ports: - \"8080:8080\" command: -i 'input_uvc.so --device /dev/video0' -o output_http.so video1: image: nimashoghi/streamer privileged: true ports: - \"8081:8080\" command: -i 'input_uvc.so --device /dev/video1' -o output_http.so","title":"Stereo Cameras"},{"location":"visual_slam/live-streaming-pi-video/#using-the-pi-camera-module","text":"Run the streamer image with the input_raspicam input plugin and the output_http output plugin. docker run -t -i --privileged -p 8080:8080 nimashoghi/streamer -i input_raspicam.so -o output_http.so Settings for the input_raspicam plugin are shown here.","title":"Using the Pi Camera Module"},{"location":"visual_slam/live-streaming-pi-video/#view-the-stream","text":"To view the stream, visit the URL http://{RASPBERRY_PI_LOCAL_IP}:8080/?action=stream in your browser. You can also open this network stream with VLC to record the stream as a .avi file.","title":"View the Stream"},{"location":"visual_slam/running-slam/","text":"Running Live SLAM To run live SLAM, you need the nimashoghi/ubuntu-xenial-orb-slam2-apps iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps image. The command given to the orb-slam2-apps image follows the following structure: <operation> <vocabulary file> [...args] Operation should be set to slam_app for offline SLAM and slam_app_live for live SLAM. The vocabulary file parameter should be set to /root/ORBvoc.txt . The rest of the arguments depend on the operation. slam_app Operation Usage: slam_app <vocabulary> [<name> <type=mono|stereo|both> <settings_mono> <settings_stereo> <left_image_folder> <right_image_folder> <times_file>]... slam_app_live Operation Usage: slam_app_live <vocabulary> [<name> <settings> <left_image_url> <right_image_url>] Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead. Running Live SLAM with MJPEG Video Input To run live SLAM with MJPEG video input, you need to do the following: - Set the operation parameter to slam_app_live . - Feed the video stream URL into the image input parameter. Example docker-compose.yml is shown below: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&param=.mjpg http://192.168.0.116:8081/?action=stream&param=.mjpg\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\" Running Live SLAM with Linux Video Device (/dev/videoN) Input Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are: - The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail). - The input video streams must be integers that indicate the number of the video input. For example, if your v Example docker-compose.yml is shown below. This example runs the livestream using the /dev/video0 and /dev/video1 devices as the left and right video inputs, respectively: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\" Running Offline SLAM To run offline SLAM, you use the slam_app operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a times.txt file which maps each image to a specific timestamp. Example docker-compose.yml for offline SLAM: version: \"3.1\" services: slam: image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest command: \"slam_app /root/ORBvoc.txt MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt\" privileged: true volumes: - \"./output/:/output/\" Monocular SLAM vs. Stereo SLAM The type parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty. Output After running offline or online SLAM, the program will output its predicted trajectory to the following file /output/{name}_keyframe_{stereo|mono}.csv . Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). View the following link for more information.","title":"Running SLAM"},{"location":"visual_slam/running-slam/#running-live-slam","text":"To run live SLAM, you need the nimashoghi/ubuntu-xenial-orb-slam2-apps iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps image. The command given to the orb-slam2-apps image follows the following structure: <operation> <vocabulary file> [...args] Operation should be set to slam_app for offline SLAM and slam_app_live for live SLAM. The vocabulary file parameter should be set to /root/ORBvoc.txt . The rest of the arguments depend on the operation.","title":"Running Live SLAM"},{"location":"visual_slam/running-slam/#slam_app-operation","text":"Usage: slam_app <vocabulary> [<name> <type=mono|stereo|both> <settings_mono> <settings_stereo> <left_image_folder> <right_image_folder> <times_file>]...","title":"slam_app Operation"},{"location":"visual_slam/running-slam/#slam_app_live-operation","text":"Usage: slam_app_live <vocabulary> [<name> <settings> <left_image_url> <right_image_url>] Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead.","title":"slam_app_live Operation"},{"location":"visual_slam/running-slam/#running-live-slam-with-mjpeg-video-input","text":"To run live SLAM with MJPEG video input, you need to do the following: - Set the operation parameter to slam_app_live . - Feed the video stream URL into the image input parameter. Example docker-compose.yml is shown below: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&param=.mjpg http://192.168.0.116:8081/?action=stream&param=.mjpg\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\"","title":"Running Live SLAM with MJPEG Video Input"},{"location":"visual_slam/running-slam/#running-live-slam-with-linux-video-device-devvideon-input","text":"Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are: - The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail). - The input video streams must be integers that indicate the number of the video input. For example, if your v Example docker-compose.yml is shown below. This example runs the livestream using the /dev/video0 and /dev/video1 devices as the left and right video inputs, respectively: version: \"3.1\" services: slam_live: image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest command: \"slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1\" environment: DISPLAY: 192.168.0.114:0.0 volumes: - \"./output/:/output/\"","title":"Running Live SLAM with Linux Video Device (/dev/videoN) Input"},{"location":"visual_slam/running-slam/#running-offline-slam","text":"To run offline SLAM, you use the slam_app operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a times.txt file which maps each image to a specific timestamp. Example docker-compose.yml for offline SLAM: version: \"3.1\" services: slam: image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest command: \"slam_app /root/ORBvoc.txt MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt\" privileged: true volumes: - \"./output/:/output/\"","title":"Running Offline SLAM"},{"location":"visual_slam/running-slam/#monocular-slam-vs-stereo-slam","text":"The type parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty.","title":"Monocular SLAM vs. Stereo SLAM"},{"location":"visual_slam/running-slam/#output","text":"After running offline or online SLAM, the program will output its predicted trajectory to the following file /output/{name}_keyframe_{stereo|mono}.csv . Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). View the following link for more information.","title":"Output"},{"location":"visual_slam/webcam-information/","text":"Webcam Information Authors: Nima Date: 8/01/2019 Below is a list of cameras we tried out. Logitech C270 Webcam Amazon Link: https://www.amazon.com/Logitech-Desktop-Widescreen-Calling-Recording/dp/B004FHO5Y6/ref=sr_1_3?keywords=Logitech+C270+Webcam&qid=1564694596&s=electronics&sr=1-3 - Resolution: 1280x720 - Pixel Size: 2.8um - Sensor Size: 3.58x2.02mm - Stock lens focal length: 4.2mm Minoru 3D Webcam (Red/Chrome) Amazon Link: https://www.amazon.com/gp/product/B001NXDGFY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Specifications: https://www.minoru3d.com/ VGA CMOS Sensor Maximum resolution 800\u00d7600 Camera baseline length 6 cm Maximum frame rate 30 fps L/R shutters don\u2019t synchronize(Max deviation 16.5 ms) Manual focus(from 10 cm to infinity) field angle 42 degrees (actual measurement)","title":"Webcam Information"},{"location":"visual_slam/webcam-information/#webcam-information","text":"Authors: Nima Date: 8/01/2019 Below is a list of cameras we tried out.","title":"Webcam Information"},{"location":"visual_slam/webcam-information/#logitech-c270-webcam","text":"Amazon Link: https://www.amazon.com/Logitech-Desktop-Widescreen-Calling-Recording/dp/B004FHO5Y6/ref=sr_1_3?keywords=Logitech+C270+Webcam&qid=1564694596&s=electronics&sr=1-3 - Resolution: 1280x720 - Pixel Size: 2.8um - Sensor Size: 3.58x2.02mm - Stock lens focal length: 4.2mm","title":"Logitech C270 Webcam"},{"location":"visual_slam/webcam-information/#minoru-3d-webcam-redchrome","text":"Amazon Link: https://www.amazon.com/gp/product/B001NXDGFY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Specifications: https://www.minoru3d.com/ VGA CMOS Sensor Maximum resolution 800\u00d7600 Camera baseline length 6 cm Maximum frame rate 30 fps L/R shutters don\u2019t synchronize(Max deviation 16.5 ms) Manual focus(from 10 cm to infinity) field angle 42 degrees (actual measurement)","title":"Minoru 3D Webcam (Red/Chrome)"}]}