<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://parallel-ml.github.io/docs/speech/deepspeech/">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Deepspeech - SARAV Documentaion</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Deepspeech on Raspberry Pi", url: "#_top", children: [
          ]},
          {title: "Run Deepspeech with Trained Model", url: "#run-deepspeech-with-trained-model", children: [
          ]},
          {title: "Making Your Own Model", url: "#making-your-own-model", children: [
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="../../search/main.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../sphinx/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../sphinx/" class="btn btn-xs btn-link">
        Sphinx
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../klauba-speech/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../klauba-speech/" class="btn btn-xs btn-link">
        Klauba Speech
      </a>
    </div>
    
  </div>

    

    <h1 id="deepspeech-on-raspberry-pi">Deepspeech on Raspberry Pi</h1>
<p><strong>Requirements: have python3 installed with pip3</strong></p>
<p>https://github.com/mozilla/DeepSpeech#using-the-python-package</p>
<h1 id="run-deepspeech-with-trained-model">Run Deepspeech with Trained Model</h1>
<p>(use python deepspeech package) </p>
<p><strong>WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi</strong></p>
<p>Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are:</p>
<p><strong>Make a virtual environment:</strong></p>
<ul>
<li>Pip3 install virtualenv if you don’t have virtualenv python package yet (or pip) version</li>
</ul>
<pre><code>virtualenv -p python3 $HOME/tmp/deepspeech-venv/
</code></pre>

<ul>
<li>Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made</li>
<li>deepspeech-venv will be the name of the environment so change that if you want a different name</li>
<li>Or just make a virtualenv how you normally do</li>
</ul>
<p><strong>Activate the virtual environment</strong></p>
<ul>
<li>Now the virtual environment is created with a bin folder with activate document</li>
</ul>
<p><code>source $HOME/tmp/deepspeech-venv/bin/activate</code></p>
<ul>
<li>This creates a virtual environment where you can install deepspeech related dependencies</li>
<li>Now install deepspeech package on your local environment</li>
</ul>
<p><code>pip3 install deepspeech</code></p>
<p><strong>Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to)</strong></p>
<ul>
<li>
<p>Linux: run this command in the directory you want to put the file: 
<code>wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz</code></p>
</li>
<li>
<p>Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory</p>
</li>
<li>
<p>Then, unzip the file using tar command 
<code>tar xvfz deepspeech-0.5.0-models.tar.gz</code></p>
</li>
<li>
<p>This creates a folder, called deepspeech-0.5.0-models</p>
</li>
<li>Now download an audio file you want the model to do speech to text recognition</li>
<li>Put this model in the preferred directory</li>
<li>
<p>Go to the preferred directory on the command line and run this command:
<code>deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav</code></p>
</li>
<li>
<p>EXCEPT: replace my_audio_file.wav with your audio file and 
--lm and --trie tags are optional</p>
</li>
<li>Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download</li>
</ul>
<h1 id="making-your-own-model">Making Your Own Model</h1>
<p>Next we tried to make our own model to see if we can reduce the model size:</p>
<p>1.) When running on a raspberry pi, go to the "connecting to the raspberry pi" docs to connect</p>
<ul>
<li>You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough</li>
</ul>
<p>2.) If you want to use a GPU, follow directions from the gpu slack channel for conection</p>
<ul>
<li>Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model:</li>
<li>Make or activate your virtualenv for deepspeech</li>
<li>Git clone DeepSpeech from the github 
 <code>git clone https://github.com/mozilla/DeepSpeech.git</code></li>
<li>Install required dependencies from requirements.txt file, Run these commands</li>
</ul>
<pre><code>cd deepspeech 
pip3 install -r requirements.txt
</code></pre>

<ul>
<li>If you are using gpu, use tensorflow gpu:</li>
</ul>
<pre><code>pip3 uninstall tensorflow
pip3 install 'tensorflow-gpu==1.13.1'
</code></pre>

<p><strong>Download voice training data from common voice: https://voice.mozilla.org/en/datasets;</strong>
- Download the Tatoeba dataset
- Go to the link, scroll down to the Tatoeba dataset, press more, and press download
- Move it to your preferrred directory
- Unzip the file 
The data is needs to be converted wav files.
The data needs to be split into train, test, and dev data
3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript
- Use  <a href="https://drive.google.com/file/d/1EUJ0jUKSdEiwxRm8-2jaLaO2mCK_Bg0G/view?usp=sharing">import.py</a> and <a href="https://drive.google.com/file/d/13tmUnlrohigVcNsxJLC2_g0bc7-eacVM/view?usp=sharing">untilA.csv</a> to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located)
- Put ‘import.py’ and ‘untilA.csv’ in same folder
- Install pydub (pydub will help convert MP3 to WAV)</p>
<p><code>pip3 install pydub</code>
- (Optional) <code>apt-get install ffmpeg</code>
- Edit import.py before you start running the code
- Change the fullpath variable to the directory that has the audio files
- For example, fullpath = ‘/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio’
- Now, run import.py by</p>
<p><code>python3 import.py</code>
- As a result, you will have the following files:
new_names.csv
train.csv
dev.csv
test.csv
<strong>‘new_names.csv’ is just a file that contains all wav file directories</strong>
- Using ./Deepspeech.py to create your own model</p>
<p><code>./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv</code></p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../sphinx/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../sphinx/" class="btn btn-xs btn-link">
        Sphinx
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../klauba-speech/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../klauba-speech/" class="btn btn-xs btn-link">
        Klauba Speech
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>