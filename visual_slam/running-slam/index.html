<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Running SLAM - distributed cloud ML</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Running SLAM";
    var mkdocs_page_input_path = "visual_slam/running-slam.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> distributed cloud ML</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Getting Started</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../getting-started/setting-up-pi/">Connecting to Raspberry Pi</a>
                </li>
                <li class="">
                    
    <a class="" href="../../getting-started/speaker-mic/">Setting Up Speaker and Mic</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Cameras</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../camera/picamera/">Using the PiCamera</a>
                </li>
                <li class="">
                    
    <a class="" href="../../camera/webcam/">Webcam video/image</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Vision System</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../vision/character/">Character Recognition</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Speech Recognition</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../speech/deepspeech/">Deepspeech</a>
                </li>
                <li class="">
                    
    <a class="" href="../../speech/sphinx/">Sphinx</a>
                </li>
                <li class="">
                    
    <a class="" href="../../speech/text-to-speech/">Text to Speech</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Mapping</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../mapping/lidar-slam/">Lidar</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Visual SLAM</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../webcam-information/">Webcam Information</a>
                </li>
                <li class="">
                    
    <a class="" href="../installing-docker-on-pi/">Installing Docker on the Raspberry Pi</a>
                </li>
                <li class="">
                    
    <a class="" href="../live-streaming-pi-video/">Live Streaming Raspberry Pi Video</a>
                </li>
                <li class="">
                    
    <a class="" href="../calibration/">Camera Calibration</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Running SLAM</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#running-live-slam">Running Live SLAM</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#slam_app-operation">slam_app Operation</a></li>
        
            <li><a class="toctree-l4" href="#slam_app_live-operation">slam_app_live Operation</a></li>
        
            <li><a class="toctree-l4" href="#running-live-slam-with-mjpeg-video-input">Running Live SLAM with MJPEG Video Input</a></li>
        
            <li><a class="toctree-l4" href="#running-live-slam-with-linux-video-device-devvideon-input">Running Live SLAM with Linux Video Device (/dev/videoN) Input</a></li>
        
            <li><a class="toctree-l4" href="#running-offline-slam">Running Offline SLAM</a></li>
        
            <li><a class="toctree-l4" href="#output">Output</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">iRobot</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../irobot/keyboard/">Keyboard Control</a>
                </li>
                <li class="">
                    
    <a class="" href="../../irobot/navi_lidar_voice/">Navigation with Lidar/Voice</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../fpga/split-networks/">FPGA</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../asplos2018/real-time/">ASPLOS 2018</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../people/people/">People</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">distributed cloud ML</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Visual SLAM &raquo;</li>
        
      
    
    <li>Running SLAM</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="running-live-slam">Running Live SLAM</h1>
<p>To run live SLAM, you need the <code>nimashoghi/ubuntu-xenial-orb-slam2-apps</code> iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the <code>nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps</code> image.</p>
<p>The command given to the <code>orb-slam2-apps</code> image follows the following structure:</p>
<p><code>&lt;operation&gt; &lt;vocabulary file&gt; [...args]</code></p>
<ul>
<li>Operation should be set to <code>slam_app</code> for offline SLAM and <code>slam_app_live</code> for live SLAM.</li>
<li>The <code>vocabulary file</code> parameter should be set to <code>/root/ORBvoc.txt</code>.</li>
<li>The rest of the arguments depend on the operation.</li>
</ul>
<h3 id="slam_app-operation"><code>slam_app</code> Operation</h3>
<p><code>Usage: slam_app &lt;vocabulary&gt; [&lt;name&gt; &lt;type=mono|stereo|both&gt; &lt;settings_mono&gt; &lt;settings_stereo&gt; &lt;left_image_folder&gt; &lt;right_image_folder&gt; &lt;times_file&gt;]...</code></p>
<h3 id="slam_app_live-operation"><code>slam_app_live</code> Operation</h3>
<p><code>Usage: slam_app_live &lt;vocabulary&gt; [&lt;name&gt; &lt;settings&gt; &lt;left_image_url&gt; &lt;right_image_url&gt;]</code></p>
<p><strong>Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead.</strong></p>
<h2 id="running-live-slam-with-mjpeg-video-input">Running Live SLAM with MJPEG Video Input</h2>
<p>To run live SLAM with MJPEG video input, you need to do the following:
- Set the <code>operation</code> parameter to <code>slam_app_live</code>.
- Feed the video stream URL into the image input parameter.</p>
<p>Example <code>docker-compose.yml</code> is shown below:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam_live:
        image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&amp;param=.mjpg http://192.168.0.116:8081/?action=stream&amp;param=.mjpg&quot;
        environment:
            DISPLAY: 192.168.0.114:0.0
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h2 id="running-live-slam-with-linux-video-device-devvideon-input">Running Live SLAM with Linux Video Device (/dev/videoN) Input</h2>
<p>Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are:
- The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail).
- The input video streams must be integers that indicate the number of the video input. For example, if your v</p>
<p>Example <code>docker-compose.yml</code> is shown below. This example runs the livestream using the <code>/dev/video0</code> and <code>/dev/video1</code> devices as the left and right video inputs, respectively:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam_live:
        image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1&quot;
        environment:
            DISPLAY: 192.168.0.114:0.0
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h2 id="running-offline-slam">Running Offline SLAM</h2>
<p>To run offline SLAM, you use the <code>slam_app</code> operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a <code>times.txt</code> file which maps each image to a specific timestamp.</p>
<p>Example <code>docker-compose.yml</code> for offline SLAM:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam:
        image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app /root/ORBvoc.txt  MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt&quot;
        privileged: true
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h3 id="monocular-slam-vs-stereo-slam">Monocular SLAM vs. Stereo SLAM</h3>
<p>The <code>type</code> parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty.</p>
<h2 id="output">Output</h2>
<p>After running offline or online SLAM, the program will output its predicted trajectory to the following file <code>/output/{name}_keyframe_{stereo|mono}.csv</code>. Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/tools">View the following link for more information.</a></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../irobot/keyboard/" class="btn btn-neutral float-right" title="Keyboard Control">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../calibration/" class="btn btn-neutral" title="Camera Calibration"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../calibration/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../irobot/keyboard/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
