<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://parallel-ml.github.io/docs/visual_slam/running-slam/">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Running SLAM - SARAV Documentaion</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Running Live SLAM", url: "#_top", children: [
              {title: "slam_app Operation", url: "#slam_app-operation" },
              {title: "slam_app_live Operation", url: "#slam_app_live-operation" },
              {title: "Running Live SLAM with MJPEG Video Input", url: "#running-live-slam-with-mjpeg-video-input" },
              {title: "Running Live SLAM with Linux Video Device (/dev/videoN) Input", url: "#running-live-slam-with-linux-video-device-devvideon-input" },
              {title: "Running Offline SLAM", url: "#running-offline-slam" },
              {title: "Output", url: "#output" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="../../search/main.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../irobot/keyboard/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../irobot/keyboard/" class="btn btn-xs btn-link">
        Keyboard Control
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../calibration/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../calibration/" class="btn btn-xs btn-link">
        Camera Calibration
      </a>
    </div>
    
  </div>

    

    <h1 id="running-live-slam">Running Live SLAM</h1>
<p>To run live SLAM, you need the <code>nimashoghi/ubuntu-xenial-orb-slam2-apps</code> iamge. If you're running this on the Raspberry Pi (which uses the arm32v7 architecture), you will need the <code>nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-apps</code> image.</p>
<p>The command given to the <code>orb-slam2-apps</code> image follows the following structure:</p>
<p><code>&lt;operation&gt; &lt;vocabulary file&gt; [...args]</code></p>
<ul>
<li>Operation should be set to <code>slam_app</code> for offline SLAM and <code>slam_app_live</code> for live SLAM.</li>
<li>The <code>vocabulary file</code> parameter should be set to <code>/root/ORBvoc.txt</code>.</li>
<li>The rest of the arguments depend on the operation.</li>
</ul>
<h3 id="slam_app-operation"><code>slam_app</code> Operation</h3>
<p><code>Usage: slam_app &lt;vocabulary&gt; [&lt;name&gt; &lt;type=mono|stereo|both&gt; &lt;settings_mono&gt; &lt;settings_stereo&gt; &lt;left_image_folder&gt; &lt;right_image_folder&gt; &lt;times_file&gt;]...</code></p>
<h3 id="slam_app_live-operation"><code>slam_app_live</code> Operation</h3>
<p><code>Usage: slam_app_live &lt;vocabulary&gt; [&lt;name&gt; &lt;settings&gt; &lt;left_image_url&gt; &lt;right_image_url&gt;]</code></p>
<p><strong>Important note: All file inputs to this app can be URLs instead. If they are URLs, they are read over the network instead.</strong></p>
<h2 id="running-live-slam-with-mjpeg-video-input">Running Live SLAM with MJPEG Video Input</h2>
<p>To run live SLAM with MJPEG video input, you need to do the following:
- Set the <code>operation</code> parameter to <code>slam_app_live</code>.
- Feed the video stream URL into the image input parameter.</p>
<p>Example <code>docker-compose.yml</code> is shown below:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam_live:
        image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml http://192.168.0.116:8080/?action=stream&amp;param=.mjpg http://192.168.0.116:8081/?action=stream&amp;param=.mjpg&quot;
        environment:
            DISPLAY: 192.168.0.114:0.0
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h2 id="running-live-slam-with-linux-video-device-devvideon-input">Running Live SLAM with Linux Video Device (/dev/videoN) Input</h2>
<p>Running live SLAM with Linux video device input is very similar to the MJPEG case. The only differences are:
- The Docker container must be ran as priviledged (or the device must be exposed explicitly, see Docker documentation for more detail).
- The input video streams must be integers that indicate the number of the video input. For example, if your v</p>
<p>Example <code>docker-compose.yml</code> is shown below. This example runs the livestream using the <code>/dev/video0</code> and <code>/dev/video1</code> devices as the left and right video inputs, respectively:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam_live:
        image: nimashoghi/ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app_live /root/ORBvoc.txt http://192.168.0.114:8080/settings-live-stereo.yml 0 1&quot;
        environment:
            DISPLAY: 192.168.0.114:0.0
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h2 id="running-offline-slam">Running Offline SLAM</h2>
<p>To run offline SLAM, you use the <code>slam_app</code> operation. In this case, instead of giving MJPEG streams (or Linux video device IDs), we give a folder containing a set of JPEG files. Additionally, we provide a <code>times.txt</code> file which maps each image to a specific timestamp.</p>
<p>Example <code>docker-compose.yml</code> for offline SLAM:</p>
<pre><code class="yaml">version: &quot;3.1&quot;
services:
    slam:
        image: nimashoghi/arm32v7-ubuntu-xenial-orb-slam2-app:latest
        command: &quot;slam_app /root/ORBvoc.txt  MH01 mono http://192.168.0.113:8080/euroc_mono.yaml http://192.168.0.113:8080/euroc_stereo.yaml http://192.168.0.113:8080/euroc_MH01/cam0/data http://192.168.0.113:8080/euroc_MH01/cam1/data http://192.168.0.113:8080/euroc_MH01/timestamps.txt&quot;
        privileged: true
        volumes:
            - &quot;./output/:/output/&quot;
</code></pre>

<h3 id="monocular-slam-vs-stereo-slam">Monocular SLAM vs. Stereo SLAM</h3>
<p>The <code>type</code> parameter decides whether we should run mono SLAM or stereo SLAM. Note that you will need to have the proper calibration settings, as well as 2 separate video inputs, if run stereo SLAM. If you're running mono SLAM, you can leave the 2nd video input as empty.</p>
<h2 id="output">Output</h2>
<p>After running offline or online SLAM, the program will output its predicted trajectory to the following file <code>/output/{name}_keyframe_{stereo|mono}.csv</code>. Many popular datasets, like the EuRoC dataset, provide ground-truth data. You can compare your predicted values to the ground-truth by calculating the absolute trajectory error (ATE) or the relative pose error (RPE). <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/tools">View the following link for more information.</a></p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../irobot/keyboard/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../irobot/keyboard/" class="btn btn-xs btn-link">
        Keyboard Control
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../calibration/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../calibration/" class="btn btn-xs btn-link">
        Camera Calibration
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="">Windmill Dark</a> theme by None (noraj).</p>
</footer>

</body>
</html>